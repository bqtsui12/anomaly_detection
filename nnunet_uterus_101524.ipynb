{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "nnunet uterus 101524",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bqtsui12/anomaly_detection/blob/main/nnunet_uterus_101524.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "import gdown\n",
        "!gdown 104dbcwIsFfT9z-YTWabZSTQkvH-7878X"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-10-15T22:31:38.218869Z",
          "iopub.execute_input": "2024-10-15T22:31:38.21953Z",
          "iopub.status.idle": "2024-10-15T22:32:17.512096Z",
          "shell.execute_reply.started": "2024-10-15T22:31:38.219487Z",
          "shell.execute_reply": "2024-10-15T22:32:17.511041Z"
        },
        "trusted": true,
        "id": "f_rXlB8gi0KY",
        "outputId": "7fbdc171-46ed-4191-e6c8-e59368391773"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.8.30)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\nDownloading...\nFrom (original): https://drive.google.com/uc?id=104dbcwIsFfT9z-YTWabZSTQkvH-7878X\nFrom (redirected): https://drive.google.com/uc?id=104dbcwIsFfT9z-YTWabZSTQkvH-7878X&confirm=t&uuid=87f29ed6-6424-43dc-b82d-aca8852c4b9f\nTo: /kaggle/working/Dataset013_Uterus.zip\n100%|███████████████████████████████████████| 2.09G/2.09G [00:20<00:00, 103MB/s]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/kaggle/working/Dataset013_Uterus.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/kaggle/working')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T22:52:19.43013Z",
          "iopub.execute_input": "2024-10-15T22:52:19.431139Z",
          "iopub.status.idle": "2024-10-15T22:52:24.928168Z",
          "shell.execute_reply.started": "2024-10-15T22:52:19.431085Z",
          "shell.execute_reply": "2024-10-15T22:52:24.927201Z"
        },
        "trusted": true,
        "id": "ppZw7iaYi0Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnunetv2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-16T16:30:02.251794Z",
          "iopub.execute_input": "2024-10-16T16:30:02.254345Z",
          "iopub.status.idle": "2024-10-16T16:30:58.524928Z",
          "shell.execute_reply.started": "2024-10-16T16:30:02.254262Z",
          "shell.execute_reply": "2024-10-16T16:30:58.523412Z"
        },
        "trusted": true,
        "id": "rnMPR53xi0Kc",
        "outputId": "3e81f757-0194-4d56-cbfc-aa947beab413"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting nnunetv2\n  Downloading nnunetv2-2.5.1.tar.gz (196 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.0/197.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (2.4.0+cpu)\nCollecting acvl-utils<0.3,>=0.2 (from nnunetv2)\n  Downloading acvl_utils-0.2.tar.gz (18 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting dynamic-network-architectures<0.4,>=0.3.1 (from nnunetv2)\n  Downloading dynamic_network_architectures-0.3.1.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (4.66.4)\nCollecting dicom2nifti (from nnunetv2)\n  Downloading dicom2nifti-2.5.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (1.14.1)\nCollecting batchgenerators>=0.25 (from nnunetv2)\n  Downloading batchgenerators-0.25.tar.gz (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (1.2.2)\nRequirement already satisfied: scikit-image>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (0.23.2)\nRequirement already satisfied: SimpleITK>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (2.4.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (2.2.3)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (0.20.3)\nRequirement already satisfied: tifffile in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (2024.5.22)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (2.32.3)\nRequirement already satisfied: nibabel in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (5.2.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (3.7.5)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from nnunetv2) (0.12.2)\nCollecting imagecodecs (from nnunetv2)\n  Downloading imagecodecs-2024.9.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nCollecting yacs (from nnunetv2)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nCollecting batchgeneratorsv2>=0.2 (from nnunetv2)\n  Downloading batchgeneratorsv2-0.2.1.tar.gz (34 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting einops (from nnunetv2)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting connected-components-3d (from acvl-utils<0.3,>=0.2->nnunetv2)\n  Downloading connected_components_3d-3.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from batchgenerators>=0.25->nnunetv2) (10.3.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from batchgenerators>=0.25->nnunetv2) (1.0.0)\nCollecting unittest2 (from batchgenerators>=0.25->nnunetv2)\n  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: threadpoolctl in /opt/conda/lib/python3.10/site-packages (from batchgenerators>=0.25->nnunetv2) (3.5.0)\nCollecting fft-conv-pytorch (from batchgeneratorsv2>=0.2->nnunetv2)\n  Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.19.3->nnunetv2) (3.3)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.19.3->nnunetv2) (2.34.1)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.19.3->nnunetv2) (21.3)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.19.3->nnunetv2) (0.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (1.12)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.2->nnunetv2) (2024.6.1)\nRequirement already satisfied: pydicom>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from dicom2nifti->nnunetv2) (3.0.1)\nCollecting python-gdcm (from dicom2nifti->nnunetv2)\n  Downloading python_gdcm-3.0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nnunetv2) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nnunetv2) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nnunetv2) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nnunetv2) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nnunetv2) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nnunetv2) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->nnunetv2) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->nnunetv2) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->nnunetv2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->nnunetv2) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->nnunetv2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->nnunetv2) (2024.8.30)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->nnunetv2) (1.4.2)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from yacs->nnunetv2) (6.0.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->nnunetv2) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.1.2->nnunetv2) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.1.2->nnunetv2) (1.3.0)\nCollecting argparse (from unittest2->batchgenerators>=0.25->nnunetv2)\n  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\nCollecting traceback2 (from unittest2->batchgenerators>=0.25->nnunetv2)\n  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting linecache2 (from traceback2->unittest2->batchgenerators>=0.25->nnunetv2)\n  Downloading linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\nDownloading dicom2nifti-2.5.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading imagecodecs-2024.9.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading connected_components_3d-3.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading fft_conv_pytorch-1.2.0-py3-none-any.whl (6.8 kB)\nDownloading python_gdcm-3.0.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\nDownloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\nDownloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\nBuilding wheels for collected packages: nnunetv2, acvl-utils, batchgenerators, batchgeneratorsv2, dynamic-network-architectures\n  Building wheel for nnunetv2 (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nnunetv2: filename=nnunetv2-2.5.1-py3-none-any.whl size=264368 sha256=5718af6b64fe660cdafe77466c3259d249199880dc214654b80abd50a454b9a5\n  Stored in directory: /root/.cache/pip/wheels/5d/d6/90/88743b341922dc9f6795742570aac83a1eaa55f77ee676a5a6\n  Building wheel for acvl-utils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for acvl-utils: filename=acvl_utils-0.2-py3-none-any.whl size=22438 sha256=2a91e6197138545920972cc2278321bae3e1136feda9ee44cc26de526b406f37\n  Stored in directory: /root/.cache/pip/wheels/ad/f0/84/52e8897591e66339bd2796681b9540b6c5e453c1461fa92a9e\n  Building wheel for batchgenerators (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for batchgenerators: filename=batchgenerators-0.25-py3-none-any.whl size=89007 sha256=b9fdeb9e9a245ead220d58ea1f1eb26b30ecd870848fd61466cca8eccdbaed36\n  Stored in directory: /root/.cache/pip/wheels/9e/b0/1b/40912fb58eb167b86cbc444ddb2e6ba382b248215295f932e2\n  Building wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for batchgeneratorsv2: filename=batchgeneratorsv2-0.2.1-py3-none-any.whl size=45185 sha256=bb385678f922be5b43e1016b434b9502027f82450009aa19cdbb32b78d2e71c8\n  Stored in directory: /root/.cache/pip/wheels/a7/20/91/33993997db216e7b946d379850c47837d2478be49377a6cb41\n  Building wheel for dynamic-network-architectures (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.3.1-py3-none-any.whl size=30051 sha256=90d342c9cdb8323ef1c14950c50e29d2193b098a8a2e9cfc85c7dc1a988dc896\n  Stored in directory: /root/.cache/pip/wheels/55/1b/13/a6419c8dbf998b9343710355ec3edc5c8e24d9b7b22eec95fb\nSuccessfully built nnunetv2 acvl-utils batchgenerators batchgeneratorsv2 dynamic-network-architectures\nInstalling collected packages: linecache2, argparse, yacs, traceback2, python-gdcm, imagecodecs, einops, connected-components-3d, unittest2, fft-conv-pytorch, dynamic-network-architectures, dicom2nifti, batchgenerators, batchgeneratorsv2, acvl-utils, nnunetv2\nSuccessfully installed acvl-utils-0.2 argparse-1.4.0 batchgenerators-0.25 batchgeneratorsv2-0.2.1 connected-components-3d-3.19.0 dicom2nifti-2.5.0 dynamic-network-architectures-0.3.1 einops-0.8.0 fft-conv-pytorch-1.2.0 imagecodecs-2024.9.22 linecache2-1.0.0 nnunetv2-2.5.1 python-gdcm-3.0.24.1 traceback2-1.4.0 unittest2-1.1.0 yacs-0.1.8\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env nnUNet_raw=/kaggle/working\n",
        "%env nnUNet_preprocessed=/kaggle/working/preprocessed\n",
        "%env nnUNet_results=/kaggle/working/results"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-16T16:31:17.780811Z",
          "iopub.execute_input": "2024-10-16T16:31:17.781291Z",
          "iopub.status.idle": "2024-10-16T16:31:17.790126Z",
          "shell.execute_reply.started": "2024-10-16T16:31:17.78125Z",
          "shell.execute_reply": "2024-10-16T16:31:17.788919Z"
        },
        "trusted": true,
        "id": "DgbUoVeyi0Kd",
        "outputId": "b3f172ba-6116-460d-ee07-eb91861cd1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "env: nnUNet_raw=/kaggle/working\nenv: nnUNet_preprocessed=/kaggle/working/preprocessed\nenv: nnUNet_results=/kaggle/working/results\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('/kaggle/working/preprocessed')\n",
        "os.makedirs('/kaggle/working/results')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T23:06:53.294063Z",
          "iopub.execute_input": "2024-10-15T23:06:53.294543Z",
          "iopub.status.idle": "2024-10-15T23:06:53.301186Z",
          "shell.execute_reply.started": "2024-10-15T23:06:53.294497Z",
          "shell.execute_reply": "2024-10-15T23:06:53.300082Z"
        },
        "trusted": true,
        "id": "bWlOJJTIi0Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.remove('/kaggle/working/Dataset013_Uterus/dataset.json')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T23:38:28.799327Z",
          "iopub.execute_input": "2024-10-15T23:38:28.79983Z",
          "iopub.status.idle": "2024-10-15T23:38:28.805593Z",
          "shell.execute_reply.started": "2024-10-15T23:38:28.799783Z",
          "shell.execute_reply": "2024-10-15T23:38:28.804574Z"
        },
        "trusted": true,
        "id": "Wg6VwWXoi0Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/file/d/1VTFoewm5fAEz7tKgLM6ocAyBtMz1RrS1/view?usp=drive_link'\n",
        "output = 'dataset.json'\n",
        "gdown.download(url=url, output=output, fuzzy=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T23:38:29.906126Z",
          "iopub.execute_input": "2024-10-15T23:38:29.906542Z",
          "iopub.status.idle": "2024-10-15T23:38:34.503667Z",
          "shell.execute_reply.started": "2024-10-15T23:38:29.906501Z",
          "shell.execute_reply": "2024-10-15T23:38:34.502578Z"
        },
        "trusted": true,
        "id": "w3kfB1NUi0Ke",
        "outputId": "266244db-6ed1-4a63-8d1d-930058ccfd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Downloading...\nFrom: https://drive.google.com/uc?id=1VTFoewm5fAEz7tKgLM6ocAyBtMz1RrS1\nTo: /kaggle/working/dataset.json\n100%|██████████| 426/426 [00:00<00:00, 1.28MB/s]\n",
          "output_type": "stream"
        },
        {
          "execution_count": 34,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'dataset.json'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.move('/kaggle/working/dataset.json', '/kaggle/working/Dataset013_Uterus/dataset.json')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T23:38:34.505732Z",
          "iopub.execute_input": "2024-10-15T23:38:34.506114Z",
          "iopub.status.idle": "2024-10-15T23:38:34.513423Z",
          "shell.execute_reply.started": "2024-10-15T23:38:34.506077Z",
          "shell.execute_reply": "2024-10-15T23:38:34.512501Z"
        },
        "trusted": true,
        "id": "pH1ewxMri0Ke",
        "outputId": "5131f8c3-ab75-4e55-a5e7-758061d1cce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 35,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/kaggle/working/Dataset013_Uterus/dataset.json'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nnUNetv2_plan_and_preprocess -d 013 --verify_dataset_integrity"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T23:38:37.289979Z",
          "iopub.execute_input": "2024-10-15T23:38:37.290571Z",
          "iopub.status.idle": "2024-10-15T23:59:30.890191Z",
          "shell.execute_reply.started": "2024-10-15T23:38:37.29052Z",
          "shell.execute_reply": "2024-10-15T23:59:30.888443Z"
        },
        "trusted": true,
        "id": "qB_bxMd6i0Kf",
        "outputId": "ab35a798-9e41-477b-92ab-c199c610a263"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Fingerprint extraction...\nDataset013_Uterus\nUsing <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n\n####################\nverify_dataset_integrity Done. \nIf you didn't see any error messages then your dataset is most likely OK!\n####################\n\nUsing <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n100%|█████████████████████████████████████████| 266/266 [01:06<00:00,  4.02it/s]\nExperiment planning...\n\n############################\nINFO: You are using the old nnU-Net default planner. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n############################\n\nAttempting to find 3d_lowres config. \nCurrent spacing: [4.4000001  0.49838708 0.49838708]. \nCurrent patch size: (16, 320, 320). \nCurrent median shape: [ 30.         557.2815534  501.94174757]\nAttempting to find 3d_lowres config. \nCurrent spacing: [4.4000001  0.51333869 0.51333869]. \nCurrent patch size: (16, 320, 320). \nCurrent median shape: [ 30.         541.05005184 487.32208502]\nAttempting to find 3d_lowres config. \nCurrent spacing: [4.4000001  0.52873886 0.52873886]. \nCurrent patch size: (20, 320, 256). \nCurrent median shape: [ 30.         525.29131247 473.12823789]\nAttempting to find 3d_lowres config. \nCurrent spacing: [4.4000001  0.54460102 0.54460102]. \nCurrent patch size: (20, 320, 256). \nCurrent median shape: [ 30.         509.9915655  459.34780377]\nAttempting to find 3d_lowres config. \nCurrent spacing: [4.4000001  0.56093905 0.56093905]. \nCurrent patch size: (20, 320, 256). \nCurrent median shape: [ 30.         495.13744224 445.96874153]\nAttempting to find 3d_lowres config. \nCurrent spacing: [4.4000001  0.57776722 0.57776722]. \nCurrent patch size: (20, 320, 256). \nCurrent median shape: [ 30.         480.71596334 432.97936071]\nDropping 3d_lowres config because the image size difference to 3d_fullres is too small. 3d_fullres: [ 30. 574. 517.], 3d_lowres: [30, 481, 433]\n2D U-Net configuration:\n{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 8, 'patch_size': (640, 640), 'median_image_size_in_voxels': array([574., 517.]), 'spacing': array([0.48387095, 0.48387095]), 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': (32, 64, 128, 256, 512, 512, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n\nUsing <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n3D fullres U-Net configuration:\n{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (16, 320, 320), 'median_image_size_in_voxels': array([ 30., 574., 517.]), 'spacing': array([4.4000001 , 0.48387095, 0.48387095]), 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': (32, 64, 128, 256, 320, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((1, 3, 3), (1, 3, 3), (1, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (1, 2, 2), (1, 2, 2), (1, 2, 2), (2, 2, 2), (2, 2, 2), (1, 2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': False}\n\nPlans were saved to /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans.json\nPreprocessing...\nPreprocessing dataset Dataset013_Uterus\nConfiguration: 2d...\n100%|█████████████████████████████████████████| 266/266 [07:20<00:00,  1.66s/it]\nConfiguration: 3d_fullres...\n100%|█████████████████████████████████████████| 266/266 [11:27<00:00,  2.59s/it]\nConfiguration: 3d_lowres...\nINFO: Configuration 3d_lowres not found in plans file nnUNetPlans.json of dataset Dataset013_Uterus. Skipping.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nnUNetv2_train 013 2d all --npz"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-16T03:15:03.870771Z",
          "iopub.execute_input": "2024-10-16T03:15:03.871498Z"
        },
        "trusted": true,
        "id": "J31KRbnci0Kf",
        "outputId": "1ded9d66-3e36-44d6-870d-beb5ee060cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n############################\nINFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n############################\n\nUsing device: cuda:0\n/opt/conda/lib/python3.10/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n\n#######################################################################\nPlease cite the following paper when using nnU-Net:\nIsensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n#######################################################################\n\n2024-10-16 03:15:10.395468: do_dummy_2d_data_aug: False\nusing pin_memory on device 0\nusing pin_memory on device 0\n2024-10-16 03:15:24.644870: Using torch.compile...\n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n\nThis is the configuration used by this training:\nConfiguration name: 2d\n {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 8, 'patch_size': [640, 640], 'median_image_size_in_voxels': [574.0, 517.0], 'spacing': [0.4838709533214569, 0.4838709533214569], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n\nThese are the global plan.json settings:\n {'dataset_name': 'Dataset013_Uterus', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [6.050005912780762, 0.4838709533214569, 0.4838709533214569], 'original_median_shape_after_transp': [24, 560, 504], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1936.4884033203125, 'mean': 198.8574676513672, 'median': 154.0, 'min': 0.0, 'percentile_00_5': 19.0, 'percentile_99_5': 819.0, 'std': 150.74197387695312}}} \n\n2024-10-16 03:15:30.827900: unpacking dataset...\n2024-10-16 03:16:31.246845: unpacking done...\n2024-10-16 03:16:31.248972: Unable to plot network architecture: nnUNet_compile is enabled!\n2024-10-16 03:16:31.270550: \n2024-10-16 03:16:31.270886: Epoch 0\n2024-10-16 03:16:31.271159: Current learning rate: 0.01\nW1016 03:16:56.477000 140129528547136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\nW1016 03:16:56.591000 140129528547136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\nW1016 03:16:56.701000 140129528547136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\nW1016 03:16:56.818000 140129528547136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\nW1016 03:16:56.930000 140129528547136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\nW1016 03:17:20.387000 140129528547136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:28.895000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:29.409000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:29.826000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:33.510000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] d0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:35.075000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:35.153000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:35.764000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:35.837000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:36.279000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:36.350000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:36.811000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:36.889000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:39.711000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:39.842000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:41.243000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:41.681000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:42.024000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:42.348000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\nW1016 03:17:44.632000 140124085282368 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\nW1016 03:20:37.294000 140129528547136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\nW1016 03:20:37.466000 140129528547136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\nW1016 03:20:37.643000 140129528547136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\nW1016 03:20:37.820000 140129528547136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\nW1016 03:20:37.993000 140129528547136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\nW1016 03:21:16.717000 140129528547136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.\n2024-10-16 03:21:30.596653: train_loss 0.1754\n2024-10-16 03:21:30.596970: val_loss 0.0332\n2024-10-16 03:21:30.597236: Pseudo dice [0.0, 0.0, 0.0, 0.0]\n2024-10-16 03:21:30.597540: Epoch time: 299.33 s\n2024-10-16 03:21:30.597726: Yayy! New best EMA pseudo Dice: 0.0\n2024-10-16 03:21:33.206687: \n2024-10-16 03:21:33.206848: Epoch 1\n2024-10-16 03:21:33.207018: Current learning rate: 0.00999\n2024-10-16 03:23:57.969245: train_loss -0.0351\n2024-10-16 03:23:57.969639: val_loss -0.1311\n2024-10-16 03:23:57.969862: Pseudo dice [0.5637, 0.0, 0.0, 0.0]\n2024-10-16 03:23:57.970003: Epoch time: 144.77 s\n2024-10-16 03:23:57.970154: Yayy! New best EMA pseudo Dice: 0.0141\n2024-10-16 03:24:01.739440: \n2024-10-16 03:24:01.739657: Epoch 2\n2024-10-16 03:24:01.739836: Current learning rate: 0.00998\n2024-10-16 03:26:26.673559: train_loss -0.1118\n2024-10-16 03:26:26.673827: val_loss -0.1348\n2024-10-16 03:26:26.674032: Pseudo dice [0.504, 0.0, 0.0, 0.0]\n2024-10-16 03:26:26.674215: Epoch time: 144.94 s\n2024-10-16 03:26:26.674324: Yayy! New best EMA pseudo Dice: 0.0253\n2024-10-16 03:26:30.142566: \n2024-10-16 03:26:30.142746: Epoch 3\n2024-10-16 03:26:30.142889: Current learning rate: 0.00997\n2024-10-16 03:28:55.200158: train_loss -0.1254\n2024-10-16 03:28:55.200653: val_loss -0.154\n2024-10-16 03:28:55.200851: Pseudo dice [0.5663, 0.0, 0.0, 0.0]\n2024-10-16 03:28:55.201024: Epoch time: 145.06 s\n2024-10-16 03:28:55.201239: Yayy! New best EMA pseudo Dice: 0.0369\n2024-10-16 03:28:58.358510: \n2024-10-16 03:28:58.358802: Epoch 4\n2024-10-16 03:28:58.358953: Current learning rate: 0.00996\n2024-10-16 03:31:23.196267: train_loss -0.1634\n2024-10-16 03:31:23.196549: val_loss -0.1929\n2024-10-16 03:31:23.196741: Pseudo dice [0.5369, 0.0, 0.5634, 0.0]\n2024-10-16 03:31:23.196903: Epoch time: 144.84 s\n2024-10-16 03:31:23.197014: Yayy! New best EMA pseudo Dice: 0.0607\n2024-10-16 03:31:26.472455: \n2024-10-16 03:31:26.472725: Epoch 5\n2024-10-16 03:31:26.472900: Current learning rate: 0.00995\n2024-10-16 03:33:51.675202: train_loss -0.1876\n2024-10-16 03:33:51.675473: val_loss -0.2442\n2024-10-16 03:33:51.675632: Pseudo dice [0.6065, 0.0, 0.5906, 0.0]\n2024-10-16 03:33:51.675741: Epoch time: 145.21 s\n2024-10-16 03:33:51.675943: Yayy! New best EMA pseudo Dice: 0.0846\n2024-10-16 03:33:54.788295: \n2024-10-16 03:33:54.788682: Epoch 6\n2024-10-16 03:33:54.788858: Current learning rate: 0.00995\n2024-10-16 03:36:19.535295: train_loss -0.2267\n2024-10-16 03:36:19.535535: val_loss -0.2496\n2024-10-16 03:36:19.535662: Pseudo dice [0.6056, 0.0, 0.5819, 0.0]\n2024-10-16 03:36:19.535796: Epoch time: 144.75 s\n2024-10-16 03:36:19.535894: Yayy! New best EMA pseudo Dice: 0.1058\n2024-10-16 03:36:22.604551: \n2024-10-16 03:36:22.604814: Epoch 7\n2024-10-16 03:36:22.604991: Current learning rate: 0.00994\n2024-10-16 03:38:47.747333: train_loss -0.2776\n2024-10-16 03:38:47.747811: val_loss -0.3373\n2024-10-16 03:38:47.748014: Pseudo dice [0.6607, 0.4944, 0.5972, 0.0]\n2024-10-16 03:38:47.748235: Epoch time: 145.15 s\n2024-10-16 03:38:47.748362: Yayy! New best EMA pseudo Dice: 0.139\n2024-10-16 03:38:50.911688: \n2024-10-16 03:38:50.912118: Epoch 8\n2024-10-16 03:38:50.912322: Current learning rate: 0.00993\n2024-10-16 03:41:16.329659: train_loss -0.3314\n2024-10-16 03:41:16.329919: val_loss -0.3792\n2024-10-16 03:41:16.330060: Pseudo dice [0.6661, 0.5786, 0.6289, 0.0]\n2024-10-16 03:41:16.330190: Epoch time: 145.42 s\n2024-10-16 03:41:16.330282: Yayy! New best EMA pseudo Dice: 0.172\n2024-10-16 03:41:19.789397: \n2024-10-16 03:41:19.789758: Epoch 9\n2024-10-16 03:41:19.789941: Current learning rate: 0.00992\n2024-10-16 03:43:44.925315: train_loss -0.3442\n2024-10-16 03:43:44.925549: val_loss -0.3851\n2024-10-16 03:43:44.925676: Pseudo dice [0.6717, 0.6114, 0.6531, 0.0]\n2024-10-16 03:43:44.925791: Epoch time: 145.14 s\n2024-10-16 03:43:44.925903: Yayy! New best EMA pseudo Dice: 0.2032\n2024-10-16 03:43:47.922901: \n2024-10-16 03:43:47.923227: Epoch 10\n2024-10-16 03:43:47.923403: Current learning rate: 0.00991\n2024-10-16 03:46:13.112247: train_loss -0.3595\n2024-10-16 03:46:13.112570: val_loss -0.3756\n2024-10-16 03:46:13.112774: Pseudo dice [0.6888, 0.6326, 0.657, 0.0]\n2024-10-16 03:46:13.112946: Epoch time: 145.19 s\n2024-10-16 03:46:13.113116: Yayy! New best EMA pseudo Dice: 0.2323\n2024-10-16 03:46:16.329049: \n2024-10-16 03:46:16.329289: Epoch 11\n2024-10-16 03:46:16.329467: Current learning rate: 0.0099\n2024-10-16 03:48:41.238385: train_loss -0.3943\n2024-10-16 03:48:41.238683: val_loss -0.4091\n2024-10-16 03:48:41.238970: Pseudo dice [0.6912, 0.6309, 0.6802, 0.0]\n2024-10-16 03:48:41.239168: Epoch time: 144.91 s\n2024-10-16 03:48:41.239324: Yayy! New best EMA pseudo Dice: 0.2592\n2024-10-16 03:48:44.557239: \n2024-10-16 03:48:44.557638: Epoch 12\n2024-10-16 03:48:44.557791: Current learning rate: 0.00989\n2024-10-16 03:51:09.411282: train_loss -0.3978\n2024-10-16 03:51:09.411579: val_loss -0.4433\n2024-10-16 03:51:09.411730: Pseudo dice [0.7448, 0.6699, 0.7079, 0.0]\n2024-10-16 03:51:09.411880: Epoch time: 144.86 s\n2024-10-16 03:51:09.412143: Yayy! New best EMA pseudo Dice: 0.2863\n2024-10-16 03:51:12.580761: \n2024-10-16 03:51:12.580955: Epoch 13\n2024-10-16 03:51:12.581109: Current learning rate: 0.00988\n2024-10-16 03:53:37.416055: train_loss -0.4191\n2024-10-16 03:53:37.416394: val_loss -0.4365\n2024-10-16 03:53:37.416544: Pseudo dice [0.6992, 0.6733, 0.7342, 0.0]\n2024-10-16 03:53:37.416660: Epoch time: 144.84 s\n2024-10-16 03:53:37.416753: Yayy! New best EMA pseudo Dice: 0.3103\n2024-10-16 03:53:40.518001: \n2024-10-16 03:53:40.518294: Epoch 14\n2024-10-16 03:53:40.518477: Current learning rate: 0.00987\n2024-10-16 03:56:05.246228: train_loss -0.4396\n2024-10-16 03:56:05.246518: val_loss -0.4419\n2024-10-16 03:56:05.246697: Pseudo dice [0.7137, 0.6958, 0.7527, 0.0]\n2024-10-16 03:56:05.246829: Epoch time: 144.73 s\n2024-10-16 03:56:05.246937: Yayy! New best EMA pseudo Dice: 0.3334\n2024-10-16 03:56:08.736677: \n2024-10-16 03:56:08.736983: Epoch 15\n2024-10-16 03:56:08.737142: Current learning rate: 0.00986\n2024-10-16 03:58:33.648984: train_loss -0.4422\n2024-10-16 03:58:33.649354: val_loss -0.4202\n2024-10-16 03:58:33.649510: Pseudo dice [0.6793, 0.6251, 0.7248, 0.0]\n2024-10-16 03:58:33.649744: Epoch time: 144.91 s\n2024-10-16 03:58:33.649844: Yayy! New best EMA pseudo Dice: 0.3508\n2024-10-16 03:58:37.234069: \n2024-10-16 03:58:37.234352: Epoch 16\n2024-10-16 03:58:37.234498: Current learning rate: 0.00986\n2024-10-16 04:01:02.431087: train_loss -0.4432\n2024-10-16 04:01:02.431444: val_loss -0.4672\n2024-10-16 04:01:02.431633: Pseudo dice [0.7402, 0.7361, 0.6955, 0.0]\n2024-10-16 04:01:02.431818: Epoch time: 145.2 s\n2024-10-16 04:01:02.432031: Yayy! New best EMA pseudo Dice: 0.37\n2024-10-16 04:01:05.766489: \n2024-10-16 04:01:05.766852: Epoch 17\n2024-10-16 04:01:05.767040: Current learning rate: 0.00985\n2024-10-16 04:03:31.161770: train_loss -0.4551\n2024-10-16 04:03:31.162025: val_loss -0.4845\n2024-10-16 04:03:31.162238: Pseudo dice [0.7623, 0.7023, 0.7581, 0.0]\n2024-10-16 04:03:31.162395: Epoch time: 145.4 s\n2024-10-16 04:03:31.162501: Yayy! New best EMA pseudo Dice: 0.3885\n2024-10-16 04:03:34.312817: \n2024-10-16 04:03:34.313182: Epoch 18\n2024-10-16 04:03:34.313369: Current learning rate: 0.00984\n2024-10-16 04:05:59.315142: train_loss -0.4743\n2024-10-16 04:05:59.315394: val_loss -0.5071\n2024-10-16 04:05:59.315587: Pseudo dice [0.7472, 0.7348, 0.781, 0.0]\n2024-10-16 04:05:59.315836: Epoch time: 145.01 s\n2024-10-16 04:05:59.315986: Yayy! New best EMA pseudo Dice: 0.4063\n2024-10-16 04:06:02.467424: \n2024-10-16 04:06:02.467682: Epoch 19\n2024-10-16 04:06:02.467857: Current learning rate: 0.00983\n2024-10-16 04:08:27.612818: train_loss -0.4865\n2024-10-16 04:08:27.613138: val_loss -0.5019\n2024-10-16 04:08:27.613309: Pseudo dice [0.7374, 0.7503, 0.7786, 0.0]\n2024-10-16 04:08:27.613452: Epoch time: 145.15 s\n2024-10-16 04:08:27.613567: Yayy! New best EMA pseudo Dice: 0.4223\n2024-10-16 04:08:30.764909: \n2024-10-16 04:08:30.765202: Epoch 20\n2024-10-16 04:08:30.765359: Current learning rate: 0.00982\n2024-10-16 04:10:55.858825: train_loss -0.4686\n2024-10-16 04:10:55.859134: val_loss -0.5117\n2024-10-16 04:10:55.859340: Pseudo dice [0.7467, 0.7149, 0.7919, 0.0]\n2024-10-16 04:10:55.859473: Epoch time: 145.1 s\n2024-10-16 04:10:55.859579: Yayy! New best EMA pseudo Dice: 0.4364\n2024-10-16 04:10:59.102457: \n2024-10-16 04:10:59.102619: Epoch 21\n2024-10-16 04:10:59.102786: Current learning rate: 0.00981\n2024-10-16 04:13:24.038754: train_loss -0.4765\n2024-10-16 04:13:24.039033: val_loss -0.4774\n2024-10-16 04:13:24.039262: Pseudo dice [0.75, 0.6993, 0.7741, 0.0]\n2024-10-16 04:13:24.039407: Epoch time: 144.94 s\n2024-10-16 04:13:24.039545: Yayy! New best EMA pseudo Dice: 0.4483\n2024-10-16 04:13:29.357330: \n2024-10-16 04:13:29.357691: Epoch 22\n2024-10-16 04:13:29.357862: Current learning rate: 0.0098\n2024-10-16 04:15:54.791686: train_loss -0.4901\n2024-10-16 04:15:54.792017: val_loss -0.5121\n2024-10-16 04:15:54.792283: Pseudo dice [0.7677, 0.7041, 0.7846, 0.0]\n2024-10-16 04:15:54.792472: Epoch time: 145.44 s\n2024-10-16 04:15:54.792626: Yayy! New best EMA pseudo Dice: 0.4599\n2024-10-16 04:15:58.172178: \n2024-10-16 04:15:58.172450: Epoch 23\n2024-10-16 04:15:58.172616: Current learning rate: 0.00979\n2024-10-16 04:18:23.026665: train_loss -0.4854\n2024-10-16 04:18:23.026962: val_loss -0.5088\n2024-10-16 04:18:23.027151: Pseudo dice [0.7571, 0.7042, 0.7727, 0.0799]\n2024-10-16 04:18:23.027289: Epoch time: 144.86 s\n2024-10-16 04:18:23.027411: Yayy! New best EMA pseudo Dice: 0.4718\n2024-10-16 04:18:26.167166: \n2024-10-16 04:18:26.167459: Epoch 24\n2024-10-16 04:18:26.167639: Current learning rate: 0.00978\n2024-10-16 04:20:51.511746: train_loss -0.5062\n2024-10-16 04:20:51.512080: val_loss -0.5247\n2024-10-16 04:20:51.512302: Pseudo dice [0.7214, 0.7484, 0.744, 0.416]\n2024-10-16 04:20:51.512482: Epoch time: 145.35 s\n2024-10-16 04:20:51.512603: Yayy! New best EMA pseudo Dice: 0.4903\n2024-10-16 04:20:54.761784: \n2024-10-16 04:20:54.762019: Epoch 25\n2024-10-16 04:20:54.762208: Current learning rate: 0.00977\n2024-10-16 04:23:20.111053: train_loss -0.5322\n2024-10-16 04:23:20.111385: val_loss -0.5193\n2024-10-16 04:23:20.111635: Pseudo dice [0.749, 0.6564, 0.7407, 0.5253]\n2024-10-16 04:23:20.111869: Epoch time: 145.35 s\n2024-10-16 04:23:20.111976: Yayy! New best EMA pseudo Dice: 0.5081\n2024-10-16 04:23:23.355411: \n2024-10-16 04:23:23.355797: Epoch 26\n2024-10-16 04:23:23.356002: Current learning rate: 0.00977\n2024-10-16 04:25:48.711325: train_loss -0.5278\n2024-10-16 04:25:48.711609: val_loss -0.5736\n2024-10-16 04:25:48.711980: Pseudo dice [0.7725, 0.751, 0.8104, 0.5135]\n2024-10-16 04:25:48.712168: Epoch time: 145.36 s\n2024-10-16 04:25:48.712274: Yayy! New best EMA pseudo Dice: 0.5285\n2024-10-16 04:25:51.941399: \n2024-10-16 04:25:51.941656: Epoch 27\n2024-10-16 04:25:51.941798: Current learning rate: 0.00976\n2024-10-16 04:28:17.150892: train_loss -0.5381\n2024-10-16 04:28:17.151248: val_loss -0.5512\n2024-10-16 04:28:17.151620: Pseudo dice [0.7796, 0.7561, 0.7523, 0.4939]\n2024-10-16 04:28:17.151750: Epoch time: 145.21 s\n2024-10-16 04:28:17.151846: Yayy! New best EMA pseudo Dice: 0.5452\n2024-10-16 04:28:20.352988: \n2024-10-16 04:28:20.353326: Epoch 28\n2024-10-16 04:28:20.353484: Current learning rate: 0.00975\n2024-10-16 04:30:45.542170: train_loss -0.5012\n2024-10-16 04:30:45.542433: val_loss -0.5433\n2024-10-16 04:30:45.542576: Pseudo dice [0.7417, 0.6918, 0.779, 0.6032]\n2024-10-16 04:30:45.542697: Epoch time: 145.19 s\n2024-10-16 04:30:45.542788: Yayy! New best EMA pseudo Dice: 0.561\n2024-10-16 04:30:48.739470: \n2024-10-16 04:30:48.739755: Epoch 29\n2024-10-16 04:30:48.740052: Current learning rate: 0.00974\n2024-10-16 04:33:13.672817: train_loss -0.5483\n2024-10-16 04:33:13.673050: val_loss -0.5693\n2024-10-16 04:33:13.673243: Pseudo dice [0.7807, 0.7337, 0.7589, 0.3367]\n2024-10-16 04:33:13.673403: Epoch time: 144.94 s\n2024-10-16 04:33:13.673574: Yayy! New best EMA pseudo Dice: 0.5702\n2024-10-16 04:33:16.818186: \n2024-10-16 04:33:16.818507: Epoch 30\n2024-10-16 04:33:16.818699: Current learning rate: 0.00973\n2024-10-16 04:35:41.775980: train_loss -0.5459\n2024-10-16 04:35:41.776404: val_loss -0.5911\n2024-10-16 04:35:41.776648: Pseudo dice [0.7596, 0.7243, 0.7929, 0.6979]\n2024-10-16 04:35:41.776820: Epoch time: 144.96 s\n2024-10-16 04:35:41.776979: Yayy! New best EMA pseudo Dice: 0.5875\n2024-10-16 04:35:45.008353: \n2024-10-16 04:35:45.008700: Epoch 31\n2024-10-16 04:35:45.008902: Current learning rate: 0.00972\n2024-10-16 04:38:09.894531: train_loss -0.5553\n2024-10-16 04:38:09.894761: val_loss -0.5894\n2024-10-16 04:38:09.894927: Pseudo dice [0.7866, 0.7784, 0.8004, 0.6062]\n2024-10-16 04:38:09.895041: Epoch time: 144.89 s\n2024-10-16 04:38:09.895158: Yayy! New best EMA pseudo Dice: 0.6031\n2024-10-16 04:38:12.940151: \n2024-10-16 04:38:12.940456: Epoch 32\n2024-10-16 04:38:12.940593: Current learning rate: 0.00971\n2024-10-16 04:40:37.930997: train_loss -0.5622\n2024-10-16 04:40:37.931304: val_loss -0.573\n2024-10-16 04:40:37.931632: Pseudo dice [0.7637, 0.7412, 0.8045, 0.6126]\n2024-10-16 04:40:37.931856: Epoch time: 144.99 s\n2024-10-16 04:40:37.931957: Yayy! New best EMA pseudo Dice: 0.6158\n2024-10-16 04:40:41.057120: \n2024-10-16 04:40:41.057447: Epoch 33\n2024-10-16 04:40:41.057627: Current learning rate: 0.0097\n2024-10-16 04:43:05.969414: train_loss -0.5663\n2024-10-16 04:43:05.969718: val_loss -0.5805\n2024-10-16 04:43:05.970013: Pseudo dice [0.7809, 0.7771, 0.7921, 0.6434]\n2024-10-16 04:43:05.970327: Epoch time: 144.92 s\n2024-10-16 04:43:05.970465: Yayy! New best EMA pseudo Dice: 0.6291\n2024-10-16 04:43:09.151912: \n2024-10-16 04:43:09.152160: Epoch 34\n2024-10-16 04:43:09.152328: Current learning rate: 0.00969\n2024-10-16 04:45:34.234680: train_loss -0.5648\n2024-10-16 04:45:34.234927: val_loss -0.5712\n2024-10-16 04:45:34.235163: Pseudo dice [0.7219, 0.7128, 0.7746, 0.658]\n2024-10-16 04:45:34.235422: Epoch time: 145.09 s\n2024-10-16 04:45:34.235559: Yayy! New best EMA pseudo Dice: 0.6378\n2024-10-16 04:45:37.427563: \n2024-10-16 04:45:37.427908: Epoch 35\n2024-10-16 04:45:37.428077: Current learning rate: 0.00968\n2024-10-16 04:48:02.382495: train_loss -0.5704\n2024-10-16 04:48:02.382751: val_loss -0.6006\n2024-10-16 04:48:02.382904: Pseudo dice [0.7632, 0.7645, 0.7979, 0.6021]\n2024-10-16 04:48:02.383027: Epoch time: 144.96 s\n2024-10-16 04:48:02.383266: Yayy! New best EMA pseudo Dice: 0.6473\n2024-10-16 04:48:05.497746: \n2024-10-16 04:48:05.498059: Epoch 36\n2024-10-16 04:48:05.498255: Current learning rate: 0.00968\n2024-10-16 04:50:30.249908: train_loss -0.5621\n2024-10-16 04:50:30.250171: val_loss -0.5965\n2024-10-16 04:50:30.250350: Pseudo dice [0.7739, 0.7641, 0.7693, 0.613]\n2024-10-16 04:50:30.250515: Epoch time: 144.76 s\n2024-10-16 04:50:30.250635: Yayy! New best EMA pseudo Dice: 0.6555\n2024-10-16 04:50:33.390223: \n2024-10-16 04:50:33.390524: Epoch 37\n2024-10-16 04:50:33.390696: Current learning rate: 0.00967\n2024-10-16 04:52:58.478546: train_loss -0.5734\n2024-10-16 04:52:58.478815: val_loss -0.5842\n2024-10-16 04:52:58.478990: Pseudo dice [0.7777, 0.7701, 0.7563, 0.5495]\n2024-10-16 04:52:58.479139: Epoch time: 145.09 s\n2024-10-16 04:52:58.479258: Yayy! New best EMA pseudo Dice: 0.6613\n2024-10-16 04:53:01.782501: \n2024-10-16 04:53:01.782793: Epoch 38\n2024-10-16 04:53:01.782936: Current learning rate: 0.00966\n2024-10-16 04:55:26.741379: train_loss -0.582\n2024-10-16 04:55:26.741679: val_loss -0.5897\n2024-10-16 04:55:26.741861: Pseudo dice [0.793, 0.7685, 0.7698, 0.5426]\n2024-10-16 04:55:26.742138: Epoch time: 144.96 s\n2024-10-16 04:55:26.742295: Yayy! New best EMA pseudo Dice: 0.667\n2024-10-16 04:55:29.857716: \n2024-10-16 04:55:29.857991: Epoch 39\n2024-10-16 04:55:29.858192: Current learning rate: 0.00965\n2024-10-16 04:57:54.858896: train_loss -0.5572\n2024-10-16 04:57:54.859234: val_loss -0.5967\n2024-10-16 04:57:54.859433: Pseudo dice [0.7826, 0.756, 0.792, 0.6032]\n2024-10-16 04:57:54.859575: Epoch time: 145.0 s\n2024-10-16 04:57:54.859698: Yayy! New best EMA pseudo Dice: 0.6737\n2024-10-16 04:57:58.049224: \n2024-10-16 04:57:58.049548: Epoch 40\n2024-10-16 04:57:58.049722: Current learning rate: 0.00964\n2024-10-16 05:00:22.917674: train_loss -0.5711\n2024-10-16 05:00:22.917996: val_loss -0.5976\n2024-10-16 05:00:22.918200: Pseudo dice [0.7571, 0.7567, 0.7758, 0.5893]\n2024-10-16 05:00:22.918334: Epoch time: 144.87 s\n2024-10-16 05:00:22.918443: Yayy! New best EMA pseudo Dice: 0.6783\n2024-10-16 05:00:28.157384: \n2024-10-16 05:00:28.157784: Epoch 41\n2024-10-16 05:00:28.157967: Current learning rate: 0.00963\n2024-10-16 05:02:53.499145: train_loss -0.5831\n2024-10-16 05:02:53.499384: val_loss -0.6071\n2024-10-16 05:02:53.499791: Pseudo dice [0.7964, 0.7792, 0.8055, 0.6191]\n2024-10-16 05:02:53.500008: Epoch time: 145.35 s\n2024-10-16 05:02:53.500153: Yayy! New best EMA pseudo Dice: 0.6855\n2024-10-16 05:02:56.636670: \n2024-10-16 05:02:56.636993: Epoch 42\n2024-10-16 05:02:56.637216: Current learning rate: 0.00962\n2024-10-16 05:05:21.751938: train_loss -0.6013\n2024-10-16 05:05:21.752202: val_loss -0.6177\n2024-10-16 05:05:21.752395: Pseudo dice [0.811, 0.7966, 0.7973, 0.6825]\n2024-10-16 05:05:21.752702: Epoch time: 145.12 s\n2024-10-16 05:05:21.752820: Yayy! New best EMA pseudo Dice: 0.6941\n2024-10-16 05:05:24.785801: \n2024-10-16 05:05:24.786216: Epoch 43\n2024-10-16 05:05:24.786408: Current learning rate: 0.00961\n2024-10-16 05:07:49.722568: train_loss -0.5827\n2024-10-16 05:07:49.722902: val_loss -0.5623\n2024-10-16 05:07:49.723070: Pseudo dice [0.7838, 0.7648, 0.798, 0.5009]\n2024-10-16 05:07:49.723302: Epoch time: 144.94 s\n2024-10-16 05:07:49.723427: Yayy! New best EMA pseudo Dice: 0.6959\n2024-10-16 05:07:52.960496: \n2024-10-16 05:07:52.960819: Epoch 44\n2024-10-16 05:07:52.960985: Current learning rate: 0.0096\n2024-10-16 05:10:18.084746: train_loss -0.5643\n2024-10-16 05:10:18.085055: val_loss -0.5874\n2024-10-16 05:10:18.085242: Pseudo dice [0.7777, 0.757, 0.7932, 0.6581]\n2024-10-16 05:10:18.085350: Epoch time: 145.13 s\n2024-10-16 05:10:18.085433: Yayy! New best EMA pseudo Dice: 0.7009\n2024-10-16 05:10:21.088754: \n2024-10-16 05:10:21.089021: Epoch 45\n2024-10-16 05:10:21.089201: Current learning rate: 0.00959\n2024-10-16 05:12:46.054507: train_loss -0.5916\n2024-10-16 05:12:46.054843: val_loss -0.6125\n2024-10-16 05:12:46.055119: Pseudo dice [0.7953, 0.7837, 0.8235, 0.6134]\n2024-10-16 05:12:46.055288: Epoch time: 144.97 s\n2024-10-16 05:12:46.055447: Yayy! New best EMA pseudo Dice: 0.7062\n2024-10-16 05:12:49.251712: \n2024-10-16 05:12:49.251940: Epoch 46\n2024-10-16 05:12:49.252155: Current learning rate: 0.00959\n2024-10-16 05:15:14.515472: train_loss -0.5965\n2024-10-16 05:15:14.515772: val_loss -0.5911\n2024-10-16 05:15:14.515938: Pseudo dice [0.7835, 0.7702, 0.8329, 0.6091]\n2024-10-16 05:15:14.516223: Epoch time: 145.27 s\n2024-10-16 05:15:14.516397: Yayy! New best EMA pseudo Dice: 0.7105\n2024-10-16 05:15:17.638212: \n2024-10-16 05:15:17.638560: Epoch 47\n2024-10-16 05:15:17.638730: Current learning rate: 0.00958\n2024-10-16 05:17:42.740295: train_loss -0.6019\n2024-10-16 05:17:42.740619: val_loss -0.6469\n2024-10-16 05:17:42.740865: Pseudo dice [0.8019, 0.7924, 0.814, 0.667]\n2024-10-16 05:17:42.741074: Epoch time: 145.11 s\n2024-10-16 05:17:42.741311: Yayy! New best EMA pseudo Dice: 0.7163\n2024-10-16 05:17:45.922426: \n2024-10-16 05:17:45.922775: Epoch 48\n2024-10-16 05:17:45.922961: Current learning rate: 0.00957\n2024-10-16 05:20:11.157519: train_loss -0.6042\n2024-10-16 05:20:11.157828: val_loss -0.6304\n2024-10-16 05:20:11.158022: Pseudo dice [0.8019, 0.7956, 0.8591, 0.6886]\n2024-10-16 05:20:11.158205: Epoch time: 145.24 s\n2024-10-16 05:20:11.158331: Yayy! New best EMA pseudo Dice: 0.7233\n2024-10-16 05:20:14.426319: \n2024-10-16 05:20:14.426722: Epoch 49\n2024-10-16 05:20:14.426911: Current learning rate: 0.00956\n2024-10-16 05:22:39.501142: train_loss -0.5922\n2024-10-16 05:22:39.501620: val_loss -0.5649\n2024-10-16 05:22:39.501837: Pseudo dice [0.8002, 0.7475, 0.8049, 0.3346]\n2024-10-16 05:22:39.502012: Epoch time: 145.08 s\n2024-10-16 05:22:42.369752: \n2024-10-16 05:22:42.370028: Epoch 50\n2024-10-16 05:22:42.370213: Current learning rate: 0.00955\n2024-10-16 05:25:07.865474: train_loss -0.5989\n2024-10-16 05:25:07.865928: val_loss -0.6191\n2024-10-16 05:25:07.866194: Pseudo dice [0.8014, 0.7758, 0.8012, 0.7313]\n2024-10-16 05:25:07.866372: Epoch time: 145.5 s\n2024-10-16 05:25:07.866522: Yayy! New best EMA pseudo Dice: 0.7241\n2024-10-16 05:25:10.924365: \n2024-10-16 05:25:10.924713: Epoch 51\n2024-10-16 05:25:10.924896: Current learning rate: 0.00954\n2024-10-16 05:27:35.882102: train_loss -0.6162\n2024-10-16 05:27:35.882610: val_loss -0.6216\n2024-10-16 05:27:35.882763: Pseudo dice [0.8081, 0.7663, 0.8141, 0.7352]\n2024-10-16 05:27:35.882874: Epoch time: 144.96 s\n2024-10-16 05:27:35.882959: Yayy! New best EMA pseudo Dice: 0.7298\n2024-10-16 05:27:38.969462: \n2024-10-16 05:27:38.969723: Epoch 52\n2024-10-16 05:27:38.970003: Current learning rate: 0.00953\n2024-10-16 05:30:04.050362: train_loss -0.599\n2024-10-16 05:30:04.050625: val_loss -0.6187\n2024-10-16 05:30:04.050763: Pseudo dice [0.8013, 0.7825, 0.8105, 0.6906]\n2024-10-16 05:30:04.050881: Epoch time: 145.08 s\n2024-10-16 05:30:04.051011: Yayy! New best EMA pseudo Dice: 0.7339\n2024-10-16 05:30:07.093687: \n2024-10-16 05:30:07.093933: Epoch 53\n2024-10-16 05:30:07.094129: Current learning rate: 0.00952\n2024-10-16 05:32:32.034256: train_loss -0.6222\n2024-10-16 05:32:32.034506: val_loss -0.6105\n2024-10-16 05:32:32.034746: Pseudo dice [0.812, 0.8355, 0.7725, 0.7115]\n2024-10-16 05:32:32.034923: Epoch time: 144.94 s\n2024-10-16 05:32:32.035163: Yayy! New best EMA pseudo Dice: 0.7388\n2024-10-16 05:32:35.097579: \n2024-10-16 05:32:35.097871: Epoch 54\n2024-10-16 05:32:35.098015: Current learning rate: 0.00951\n2024-10-16 05:34:59.935929: train_loss -0.614\n2024-10-16 05:34:59.936233: val_loss -0.6205\n2024-10-16 05:34:59.936396: Pseudo dice [0.8142, 0.7796, 0.8325, 0.7723]\n2024-10-16 05:34:59.936511: Epoch time: 144.84 s\n2024-10-16 05:34:59.936621: Yayy! New best EMA pseudo Dice: 0.7449\n2024-10-16 05:35:02.980559: \n2024-10-16 05:35:02.980767: Epoch 55\n2024-10-16 05:35:02.980937: Current learning rate: 0.0095\n2024-10-16 05:37:28.104028: train_loss -0.6076\n2024-10-16 05:37:28.104481: val_loss -0.6157\n2024-10-16 05:37:28.104618: Pseudo dice [0.8101, 0.7702, 0.8155, 0.7148]\n2024-10-16 05:37:28.104719: Epoch time: 145.13 s\n2024-10-16 05:37:28.104797: Yayy! New best EMA pseudo Dice: 0.7482\n2024-10-16 05:37:31.198026: \n2024-10-16 05:37:31.198351: Epoch 56\n2024-10-16 05:37:31.198518: Current learning rate: 0.00949\n2024-10-16 05:39:56.201221: train_loss -0.604\n2024-10-16 05:39:56.201469: val_loss -0.6387\n2024-10-16 05:39:56.201644: Pseudo dice [0.8155, 0.8197, 0.8517, 0.7119]\n2024-10-16 05:39:56.201799: Epoch time: 145.01 s\n2024-10-16 05:39:56.201990: Yayy! New best EMA pseudo Dice: 0.7533\n2024-10-16 05:39:59.224096: \n2024-10-16 05:39:59.224403: Epoch 57\n2024-10-16 05:39:59.224584: Current learning rate: 0.00949\n2024-10-16 05:42:24.305477: train_loss -0.6115\n2024-10-16 05:42:24.305882: val_loss -0.6012\n2024-10-16 05:42:24.306137: Pseudo dice [0.8054, 0.755, 0.8019, 0.6285]\n2024-10-16 05:42:24.306319: Epoch time: 145.08 s\n2024-10-16 05:42:26.673465: \n2024-10-16 05:42:26.673686: Epoch 58\n2024-10-16 05:42:26.673862: Current learning rate: 0.00948\n2024-10-16 05:44:51.772642: train_loss -0.6188\n2024-10-16 05:44:51.772890: val_loss -0.6336\n2024-10-16 05:44:51.773028: Pseudo dice [0.7941, 0.7919, 0.8097, 0.7686]\n2024-10-16 05:44:51.773200: Epoch time: 145.1 s\n2024-10-16 05:44:51.773350: Yayy! New best EMA pseudo Dice: 0.7566\n2024-10-16 05:44:55.435949: \n2024-10-16 05:44:55.436335: Epoch 59\n2024-10-16 05:44:55.436495: Current learning rate: 0.00947\n2024-10-16 05:47:20.680471: train_loss -0.5948\n2024-10-16 05:47:20.680768: val_loss -0.6039\n2024-10-16 05:47:20.681080: Pseudo dice [0.8047, 0.7265, 0.7936, 0.7459]\n2024-10-16 05:47:20.681389: Epoch time: 145.25 s\n2024-10-16 05:47:20.681505: Yayy! New best EMA pseudo Dice: 0.7577\n2024-10-16 05:47:24.728557: \n2024-10-16 05:47:24.728927: Epoch 60\n2024-10-16 05:47:24.729083: Current learning rate: 0.00946\n2024-10-16 05:49:50.027655: train_loss -0.6066\n2024-10-16 05:49:50.027930: val_loss -0.6271\n2024-10-16 05:49:50.028127: Pseudo dice [0.7735, 0.7567, 0.7883, 0.7493]\n2024-10-16 05:49:50.028271: Epoch time: 145.3 s\n2024-10-16 05:49:50.028406: Yayy! New best EMA pseudo Dice: 0.7586\n2024-10-16 05:49:53.432799: \n2024-10-16 05:49:53.433219: Epoch 61\n2024-10-16 05:49:53.433426: Current learning rate: 0.00945\n2024-10-16 05:52:18.817764: train_loss -0.6089\n2024-10-16 05:52:18.818044: val_loss -0.6021\n2024-10-16 05:52:18.818247: Pseudo dice [0.7965, 0.8115, 0.7949, 0.657]\n2024-10-16 05:52:18.818391: Epoch time: 145.39 s\n2024-10-16 05:52:18.818508: Yayy! New best EMA pseudo Dice: 0.7593\n2024-10-16 05:52:22.001650: \n2024-10-16 05:52:22.002027: Epoch 62\n2024-10-16 05:52:22.002234: Current learning rate: 0.00944\n2024-10-16 05:54:47.274522: train_loss -0.615\n2024-10-16 05:54:47.274981: val_loss -0.6656\n2024-10-16 05:54:47.275163: Pseudo dice [0.8238, 0.8137, 0.808, 0.7753]\n2024-10-16 05:54:47.275311: Epoch time: 145.28 s\n2024-10-16 05:54:47.275461: Yayy! New best EMA pseudo Dice: 0.7639\n2024-10-16 05:54:50.482044: \n2024-10-16 05:54:50.482405: Epoch 63\n2024-10-16 05:54:50.482594: Current learning rate: 0.00943\n2024-10-16 05:57:15.676555: train_loss -0.6333\n2024-10-16 05:57:15.676861: val_loss -0.6575\n2024-10-16 05:57:15.676998: Pseudo dice [0.8042, 0.8127, 0.8138, 0.8016]\n2024-10-16 05:57:15.677134: Epoch time: 145.2 s\n2024-10-16 05:57:15.677295: Yayy! New best EMA pseudo Dice: 0.7683\n2024-10-16 05:57:18.731738: \n2024-10-16 05:57:18.731934: Epoch 64\n2024-10-16 05:57:18.732131: Current learning rate: 0.00942\n2024-10-16 05:59:44.051439: train_loss -0.64\n2024-10-16 05:59:44.051772: val_loss -0.6376\n2024-10-16 05:59:44.052052: Pseudo dice [0.8061, 0.8021, 0.843, 0.7187]\n2024-10-16 05:59:44.052323: Epoch time: 145.32 s\n2024-10-16 05:59:44.052488: Yayy! New best EMA pseudo Dice: 0.7707\n2024-10-16 05:59:47.338602: \n2024-10-16 05:59:47.338972: Epoch 65\n2024-10-16 05:59:47.339129: Current learning rate: 0.00941\n2024-10-16 06:02:12.764593: train_loss -0.6447\n2024-10-16 06:02:12.764854: val_loss -0.6246\n2024-10-16 06:02:12.765047: Pseudo dice [0.8097, 0.7921, 0.7828, 0.71]\n2024-10-16 06:02:12.765270: Epoch time: 145.43 s\n2024-10-16 06:02:12.765405: Yayy! New best EMA pseudo Dice: 0.771\n2024-10-16 06:02:16.027894: \n2024-10-16 06:02:16.028226: Epoch 66\n2024-10-16 06:02:16.028411: Current learning rate: 0.0094\n2024-10-16 06:04:41.348588: train_loss -0.6169\n2024-10-16 06:04:41.349146: val_loss -0.6208\n2024-10-16 06:04:41.349360: Pseudo dice [0.8038, 0.7124, 0.8364, 0.7021]\n2024-10-16 06:04:41.349500: Epoch time: 145.32 s\n2024-10-16 06:04:43.842579: \n2024-10-16 06:04:43.842986: Epoch 67\n2024-10-16 06:04:43.843238: Current learning rate: 0.00939\n2024-10-16 06:07:09.050242: train_loss -0.6278\n2024-10-16 06:07:09.050508: val_loss -0.6554\n2024-10-16 06:07:09.050719: Pseudo dice [0.8136, 0.7894, 0.813, 0.7525]\n2024-10-16 06:07:09.050924: Epoch time: 145.21 s\n2024-10-16 06:07:09.051074: Yayy! New best EMA pseudo Dice: 0.7725\n2024-10-16 06:07:12.428183: \n2024-10-16 06:07:12.428513: Epoch 68\n2024-10-16 06:07:12.428665: Current learning rate: 0.00939\n2024-10-16 06:09:37.270288: train_loss -0.6116\n2024-10-16 06:09:37.270587: val_loss -0.6216\n2024-10-16 06:09:37.270774: Pseudo dice [0.8158, 0.7877, 0.8525, 0.6881]\n2024-10-16 06:09:37.270911: Epoch time: 144.84 s\n2024-10-16 06:09:37.271007: Yayy! New best EMA pseudo Dice: 0.7738\n2024-10-16 06:09:40.713545: \n2024-10-16 06:09:40.713896: Epoch 69\n2024-10-16 06:09:40.714116: Current learning rate: 0.00938\n2024-10-16 06:12:05.444911: train_loss -0.6057\n2024-10-16 06:12:05.445207: val_loss -0.6228\n2024-10-16 06:12:05.445366: Pseudo dice [0.7894, 0.7491, 0.8414, 0.7142]\n2024-10-16 06:12:05.445527: Epoch time: 144.73 s\n2024-10-16 06:12:08.018468: \n2024-10-16 06:12:08.018819: Epoch 70\n2024-10-16 06:12:08.018974: Current learning rate: 0.00937\n2024-10-16 06:14:32.844319: train_loss -0.6128\n2024-10-16 06:14:32.844613: val_loss -0.6638\n2024-10-16 06:14:32.844872: Pseudo dice [0.8041, 0.8186, 0.8186, 0.7861]\n2024-10-16 06:14:32.845023: Epoch time: 144.83 s\n2024-10-16 06:14:32.845248: Yayy! New best EMA pseudo Dice: 0.7771\n2024-10-16 06:14:36.291264: \n2024-10-16 06:14:36.291698: Epoch 71\n2024-10-16 06:14:36.291915: Current learning rate: 0.00936\n2024-10-16 06:17:01.128578: train_loss -0.5957\n2024-10-16 06:17:01.129010: val_loss -0.6423\n2024-10-16 06:17:01.129245: Pseudo dice [0.8188, 0.8061, 0.8276, 0.7407]\n2024-10-16 06:17:01.129473: Epoch time: 144.84 s\n2024-10-16 06:17:01.129598: Yayy! New best EMA pseudo Dice: 0.7792\n2024-10-16 06:17:04.572345: \n2024-10-16 06:17:04.572702: Epoch 72\n2024-10-16 06:17:04.572890: Current learning rate: 0.00935\n2024-10-16 06:19:29.363453: train_loss -0.6331\n2024-10-16 06:19:29.363716: val_loss -0.6526\n2024-10-16 06:19:29.363899: Pseudo dice [0.816, 0.7893, 0.8479, 0.7476]\n2024-10-16 06:19:29.364033: Epoch time: 144.79 s\n2024-10-16 06:19:29.364195: Yayy! New best EMA pseudo Dice: 0.7813\n2024-10-16 06:19:32.777218: \n2024-10-16 06:19:32.777542: Epoch 73\n2024-10-16 06:19:32.777730: Current learning rate: 0.00934\n2024-10-16 06:21:57.401361: train_loss -0.6318\n2024-10-16 06:21:57.401670: val_loss -0.6613\n2024-10-16 06:21:57.401846: Pseudo dice [0.8084, 0.7746, 0.8357, 0.7395]\n2024-10-16 06:21:57.401968: Epoch time: 144.63 s\n2024-10-16 06:21:57.402074: Yayy! New best EMA pseudo Dice: 0.7821\n2024-10-16 06:22:00.727089: \n2024-10-16 06:22:00.727415: Epoch 74\n2024-10-16 06:22:00.727571: Current learning rate: 0.00933\n2024-10-16 06:24:25.423376: train_loss -0.6338\n2024-10-16 06:24:25.423768: val_loss -0.6528\n2024-10-16 06:24:25.423918: Pseudo dice [0.8285, 0.7795, 0.8603, 0.6948]\n2024-10-16 06:24:25.424033: Epoch time: 144.7 s\n2024-10-16 06:24:25.424185: Yayy! New best EMA pseudo Dice: 0.783\n2024-10-16 06:24:28.878529: \n2024-10-16 06:24:28.878832: Epoch 75\n2024-10-16 06:24:28.879032: Current learning rate: 0.00932\n2024-10-16 06:26:53.541075: train_loss -0.6246\n2024-10-16 06:26:53.541512: val_loss -0.6556\n2024-10-16 06:26:53.541689: Pseudo dice [0.8081, 0.7825, 0.8334, 0.8178]\n2024-10-16 06:26:53.541844: Epoch time: 144.67 s\n2024-10-16 06:26:53.541944: Yayy! New best EMA pseudo Dice: 0.7857\n2024-10-16 06:26:56.949091: \n2024-10-16 06:26:56.949280: Epoch 76\n2024-10-16 06:26:56.949453: Current learning rate: 0.00931\n2024-10-16 06:29:21.880530: train_loss -0.6265\n2024-10-16 06:29:21.880806: val_loss -0.6459\n2024-10-16 06:29:21.881011: Pseudo dice [0.7808, 0.8085, 0.7834, 0.7781]\n2024-10-16 06:29:21.881186: Epoch time: 144.93 s\n2024-10-16 06:29:21.881337: Yayy! New best EMA pseudo Dice: 0.7859\n2024-10-16 06:29:25.398870: \n2024-10-16 06:29:25.399227: Epoch 77\n2024-10-16 06:29:25.399395: Current learning rate: 0.0093\n2024-10-16 06:31:50.627690: train_loss -0.6312\n2024-10-16 06:31:50.628007: val_loss -0.673\n2024-10-16 06:31:50.628270: Pseudo dice [0.8395, 0.8189, 0.8529, 0.7342]\n2024-10-16 06:31:50.628459: Epoch time: 145.23 s\n2024-10-16 06:31:50.628866: Yayy! New best EMA pseudo Dice: 0.7885\n2024-10-16 06:31:54.235571: \n2024-10-16 06:31:54.235842: Epoch 78\n2024-10-16 06:31:54.236053: Current learning rate: 0.0093\n2024-10-16 06:34:19.538400: train_loss -0.6201\n2024-10-16 06:34:19.538707: val_loss -0.6496\n2024-10-16 06:34:19.538993: Pseudo dice [0.8258, 0.7904, 0.8479, 0.7485]\n2024-10-16 06:34:19.539196: Epoch time: 145.31 s\n2024-10-16 06:34:19.539356: Yayy! New best EMA pseudo Dice: 0.79\n2024-10-16 06:34:23.352724: \n2024-10-16 06:34:23.353062: Epoch 79\n2024-10-16 06:34:23.353289: Current learning rate: 0.00929\n2024-10-16 06:36:49.325197: train_loss -0.6377\n2024-10-16 06:36:49.325493: val_loss -0.6511\n2024-10-16 06:36:49.325689: Pseudo dice [0.8239, 0.81, 0.8137, 0.7607]\n2024-10-16 06:36:49.325933: Epoch time: 145.98 s\n2024-10-16 06:36:49.326120: Yayy! New best EMA pseudo Dice: 0.7912\n2024-10-16 06:36:52.772496: \n2024-10-16 06:36:52.772911: Epoch 80\n2024-10-16 06:36:52.773134: Current learning rate: 0.00928\n2024-10-16 06:39:17.790324: train_loss -0.6214\n2024-10-16 06:39:17.790737: val_loss -0.6594\n2024-10-16 06:39:17.790968: Pseudo dice [0.83, 0.7985, 0.8585, 0.7287]\n2024-10-16 06:39:17.791279: Epoch time: 145.02 s\n2024-10-16 06:39:17.791440: Yayy! New best EMA pseudo Dice: 0.7924\n2024-10-16 06:39:21.405854: \n2024-10-16 06:39:21.406226: Epoch 81\n2024-10-16 06:39:21.406390: Current learning rate: 0.00927\n2024-10-16 06:41:46.688298: train_loss -0.6475\n2024-10-16 06:41:46.688565: val_loss -0.6391\n2024-10-16 06:41:46.688776: Pseudo dice [0.8216, 0.8001, 0.8082, 0.7584]\n2024-10-16 06:41:46.688960: Epoch time: 145.29 s\n2024-10-16 06:41:46.689082: Yayy! New best EMA pseudo Dice: 0.7929\n2024-10-16 06:41:50.116165: \n2024-10-16 06:41:50.116467: Epoch 82\n2024-10-16 06:41:50.116636: Current learning rate: 0.00926\n2024-10-16 06:44:15.379485: train_loss -0.6288\n2024-10-16 06:44:15.379760: val_loss -0.624\n2024-10-16 06:44:15.379904: Pseudo dice [0.8202, 0.8036, 0.7934, 0.769]\n2024-10-16 06:44:15.380112: Epoch time: 145.27 s\n2024-10-16 06:44:15.380290: Yayy! New best EMA pseudo Dice: 0.7933\n2024-10-16 06:44:18.677479: \n2024-10-16 06:44:18.677890: Epoch 83\n2024-10-16 06:44:18.678074: Current learning rate: 0.00925\n2024-10-16 06:46:43.693156: train_loss -0.6337\n2024-10-16 06:46:43.693474: val_loss -0.6412\n2024-10-16 06:46:43.693683: Pseudo dice [0.8262, 0.8052, 0.8194, 0.6616]\n2024-10-16 06:46:43.693827: Epoch time: 145.02 s\n2024-10-16 06:46:46.141633: \n2024-10-16 06:46:46.142019: Epoch 84\n2024-10-16 06:46:46.142257: Current learning rate: 0.00924\n2024-10-16 06:49:11.485471: train_loss -0.6306\n2024-10-16 06:49:11.485742: val_loss -0.5931\n2024-10-16 06:49:11.485887: Pseudo dice [0.8062, 0.7601, 0.8168, 0.5726]\n2024-10-16 06:49:11.486183: Epoch time: 145.35 s\n2024-10-16 06:49:13.903021: \n2024-10-16 06:49:13.903385: Epoch 85\n2024-10-16 06:49:13.903549: Current learning rate: 0.00923\n2024-10-16 06:51:39.296471: train_loss -0.6276\n2024-10-16 06:51:39.296823: val_loss -0.641\n2024-10-16 06:51:39.297004: Pseudo dice [0.8012, 0.7837, 0.8349, 0.6951]\n2024-10-16 06:51:39.297167: Epoch time: 145.4 s\n2024-10-16 06:51:41.681847: \n2024-10-16 06:51:41.682184: Epoch 86\n2024-10-16 06:51:41.682338: Current learning rate: 0.00922\n2024-10-16 06:54:06.684052: train_loss -0.6295\n2024-10-16 06:54:06.684427: val_loss -0.6307\n2024-10-16 06:54:06.684660: Pseudo dice [0.8093, 0.7487, 0.8374, 0.6331]\n2024-10-16 06:54:06.684819: Epoch time: 145.01 s\n2024-10-16 06:54:09.128489: \n2024-10-16 06:54:09.128942: Epoch 87\n2024-10-16 06:54:09.129158: Current learning rate: 0.00921\n2024-10-16 06:56:34.200950: train_loss -0.6235\n2024-10-16 06:56:34.201278: val_loss -0.6256\n2024-10-16 06:56:34.201470: Pseudo dice [0.8124, 0.7812, 0.8063, 0.6329]\n2024-10-16 06:56:34.201689: Epoch time: 145.08 s\n2024-10-16 06:56:36.645336: \n2024-10-16 06:56:36.645713: Epoch 88\n2024-10-16 06:56:36.645905: Current learning rate: 0.0092\n2024-10-16 06:59:01.648823: train_loss -0.6319\n2024-10-16 06:59:01.649275: val_loss -0.6356\n2024-10-16 06:59:01.649534: Pseudo dice [0.7999, 0.802, 0.8042, 0.7501]\n2024-10-16 06:59:01.649730: Epoch time: 145.01 s\n2024-10-16 06:59:04.126457: \n2024-10-16 06:59:04.126879: Epoch 89\n2024-10-16 06:59:04.127120: Current learning rate: 0.0092\n2024-10-16 07:01:29.159726: train_loss -0.6373\n2024-10-16 07:01:29.160013: val_loss -0.6592\n2024-10-16 07:01:29.160341: Pseudo dice [0.8214, 0.7802, 0.8391, 0.8168]\n2024-10-16 07:01:29.160512: Epoch time: 145.04 s\n2024-10-16 07:01:31.773307: \n2024-10-16 07:01:31.773701: Epoch 90\n2024-10-16 07:01:31.773906: Current learning rate: 0.00919\n2024-10-16 07:03:57.021254: train_loss -0.6345\n2024-10-16 07:03:57.021655: val_loss -0.6639\n2024-10-16 07:03:57.022017: Pseudo dice [0.8221, 0.79, 0.8495, 0.7064]\n2024-10-16 07:03:57.022292: Epoch time: 145.25 s\n2024-10-16 07:03:59.472944: \n2024-10-16 07:03:59.473185: Epoch 91\n2024-10-16 07:03:59.473412: Current learning rate: 0.00918\n2024-10-16 07:06:24.485163: train_loss -0.661\n2024-10-16 07:06:24.485424: val_loss -0.6827\n2024-10-16 07:06:24.485569: Pseudo dice [0.8281, 0.8097, 0.8645, 0.8149]\n2024-10-16 07:06:24.485686: Epoch time: 145.01 s\n2024-10-16 07:06:26.800255: \n2024-10-16 07:06:26.800569: Epoch 92\n2024-10-16 07:06:26.800718: Current learning rate: 0.00917\n2024-10-16 07:08:51.980623: train_loss -0.6496\n2024-10-16 07:08:51.980916: val_loss -0.7055\n2024-10-16 07:08:51.981143: Pseudo dice [0.8302, 0.8067, 0.8568, 0.8275]\n2024-10-16 07:08:51.981310: Epoch time: 145.18 s\n2024-10-16 07:08:51.981414: Yayy! New best EMA pseudo Dice: 0.7938\n2024-10-16 07:08:55.196593: \n2024-10-16 07:08:55.196891: Epoch 93\n2024-10-16 07:08:55.197083: Current learning rate: 0.00916\n2024-10-16 07:11:20.152175: train_loss -0.6218\n2024-10-16 07:11:20.152578: val_loss -0.6025\n2024-10-16 07:11:20.152746: Pseudo dice [0.8042, 0.7737, 0.8467, 0.5418]\n2024-10-16 07:11:20.152877: Epoch time: 144.96 s\n2024-10-16 07:11:22.486996: \n2024-10-16 07:11:22.487315: Epoch 94\n2024-10-16 07:11:22.487473: Current learning rate: 0.00915\n2024-10-16 07:13:47.603251: train_loss -0.6202\n2024-10-16 07:13:47.603599: val_loss -0.655\n2024-10-16 07:13:47.603812: Pseudo dice [0.816, 0.7917, 0.8497, 0.7286]\n2024-10-16 07:13:47.603972: Epoch time: 145.12 s\n2024-10-16 07:13:49.992734: \n2024-10-16 07:13:49.993083: Epoch 95\n2024-10-16 07:13:49.993259: Current learning rate: 0.00914\n2024-10-16 07:16:14.948491: train_loss -0.6519\n2024-10-16 07:16:14.948885: val_loss -0.6516\n2024-10-16 07:16:14.949113: Pseudo dice [0.8182, 0.8112, 0.8606, 0.7585]\n2024-10-16 07:16:14.949290: Epoch time: 144.96 s\n2024-10-16 07:16:17.384238: \n2024-10-16 07:16:17.384529: Epoch 96\n2024-10-16 07:16:17.384748: Current learning rate: 0.00913\n2024-10-16 07:18:42.557368: train_loss -0.6184\n2024-10-16 07:18:42.557652: val_loss -0.6678\n2024-10-16 07:18:42.557933: Pseudo dice [0.8239, 0.796, 0.8403, 0.7943]\n2024-10-16 07:18:42.558138: Epoch time: 145.18 s\n2024-10-16 07:18:42.558286: Yayy! New best EMA pseudo Dice: 0.7938\n2024-10-16 07:18:45.955796: \n2024-10-16 07:18:45.956066: Epoch 97\n2024-10-16 07:18:45.956277: Current learning rate: 0.00912\n2024-10-16 07:21:10.894031: train_loss -0.6284\n2024-10-16 07:21:10.894477: val_loss -0.655\n2024-10-16 07:21:10.894642: Pseudo dice [0.8272, 0.8038, 0.8656, 0.8115]\n2024-10-16 07:21:10.894763: Epoch time: 144.94 s\n2024-10-16 07:21:10.894856: Yayy! New best EMA pseudo Dice: 0.7971\n2024-10-16 07:21:14.261503: \n2024-10-16 07:21:14.261879: Epoch 98\n2024-10-16 07:21:14.262048: Current learning rate: 0.00911\n2024-10-16 07:23:39.155619: train_loss -0.661\n2024-10-16 07:23:39.156024: val_loss -0.6419\n2024-10-16 07:23:39.156274: Pseudo dice [0.8261, 0.7597, 0.854, 0.7535]\n2024-10-16 07:23:39.156447: Epoch time: 144.9 s\n2024-10-16 07:23:39.156593: Yayy! New best EMA pseudo Dice: 0.7973\n2024-10-16 07:23:42.400730: \n2024-10-16 07:23:42.401133: Epoch 99\n2024-10-16 07:23:42.401358: Current learning rate: 0.0091\n2024-10-16 07:26:07.292727: train_loss -0.6648\n2024-10-16 07:26:07.292984: val_loss -0.6601\n2024-10-16 07:26:07.293195: Pseudo dice [0.8281, 0.8188, 0.8349, 0.8071]\n2024-10-16 07:26:07.293375: Epoch time: 144.9 s\n2024-10-16 07:26:08.128391: Yayy! New best EMA pseudo Dice: 0.7998\n2024-10-16 07:26:12.454185: \n2024-10-16 07:26:12.454584: Epoch 100\n2024-10-16 07:26:12.454791: Current learning rate: 0.0091\n2024-10-16 07:28:37.721540: train_loss -0.6517\n2024-10-16 07:28:37.721934: val_loss -0.667\n2024-10-16 07:28:37.722166: Pseudo dice [0.8227, 0.8207, 0.8585, 0.7615]\n2024-10-16 07:28:37.722381: Epoch time: 145.27 s\n2024-10-16 07:28:37.722535: Yayy! New best EMA pseudo Dice: 0.8014\n2024-10-16 07:28:40.936175: \n2024-10-16 07:28:40.936496: Epoch 101\n2024-10-16 07:28:40.936679: Current learning rate: 0.00909\n2024-10-16 07:31:06.075726: train_loss -0.6303\n2024-10-16 07:31:06.076286: val_loss -0.6583\n2024-10-16 07:31:06.076488: Pseudo dice [0.8162, 0.7798, 0.837, 0.7966]\n2024-10-16 07:31:06.076618: Epoch time: 145.14 s\n2024-10-16 07:31:06.076719: Yayy! New best EMA pseudo Dice: 0.802\n2024-10-16 07:31:09.372728: \n2024-10-16 07:31:09.373179: Epoch 102\n2024-10-16 07:31:09.373389: Current learning rate: 0.00908\n2024-10-16 07:33:34.408912: train_loss -0.6319\n2024-10-16 07:33:34.409481: val_loss -0.6394\n2024-10-16 07:33:34.409754: Pseudo dice [0.8122, 0.821, 0.8605, 0.6544]\n2024-10-16 07:33:34.409901: Epoch time: 145.04 s\n2024-10-16 07:33:36.750215: \n2024-10-16 07:33:36.750571: Epoch 103\n2024-10-16 07:33:36.750739: Current learning rate: 0.00907\n2024-10-16 07:36:01.671446: train_loss -0.6357\n2024-10-16 07:36:01.671759: val_loss -0.5905\n2024-10-16 07:36:01.671943: Pseudo dice [0.7945, 0.7318, 0.8099, 0.5308]\n2024-10-16 07:36:01.672148: Epoch time: 144.92 s\n2024-10-16 07:36:03.984481: \n2024-10-16 07:36:03.984730: Epoch 104\n2024-10-16 07:36:03.984900: Current learning rate: 0.00906\n2024-10-16 07:38:28.557911: train_loss -0.6069\n2024-10-16 07:38:28.558298: val_loss -0.608\n2024-10-16 07:38:28.558526: Pseudo dice [0.7979, 0.7751, 0.8273, 0.7604]\n2024-10-16 07:38:28.558710: Epoch time: 144.58 s\n2024-10-16 07:38:30.787729: \n2024-10-16 07:38:30.788027: Epoch 105\n2024-10-16 07:38:30.788203: Current learning rate: 0.00905\n2024-10-16 07:40:55.448410: train_loss -0.617\n2024-10-16 07:40:55.448891: val_loss -0.6421\n2024-10-16 07:40:55.449169: Pseudo dice [0.8131, 0.7895, 0.834, 0.7354]\n2024-10-16 07:40:55.449375: Epoch time: 144.66 s\n2024-10-16 07:40:57.684473: \n2024-10-16 07:40:57.684736: Epoch 106\n2024-10-16 07:40:57.684876: Current learning rate: 0.00904\n2024-10-16 07:43:22.258433: train_loss -0.635\n2024-10-16 07:43:22.258739: val_loss -0.6413\n2024-10-16 07:43:22.258891: Pseudo dice [0.8239, 0.8148, 0.8576, 0.7133]\n2024-10-16 07:43:22.259009: Epoch time: 144.58 s\n2024-10-16 07:43:24.595736: \n2024-10-16 07:43:24.595924: Epoch 107\n2024-10-16 07:43:24.596105: Current learning rate: 0.00903\n2024-10-16 07:45:49.186476: train_loss -0.6058\n2024-10-16 07:45:49.186749: val_loss -0.6626\n2024-10-16 07:45:49.186931: Pseudo dice [0.8212, 0.8037, 0.8634, 0.6509]\n2024-10-16 07:45:49.187125: Epoch time: 144.59 s\n2024-10-16 07:45:51.450565: \n2024-10-16 07:45:51.450912: Epoch 108\n2024-10-16 07:45:51.451148: Current learning rate: 0.00902\n2024-10-16 07:48:16.082815: train_loss -0.6304\n2024-10-16 07:48:16.083078: val_loss -0.6655\n2024-10-16 07:48:16.083293: Pseudo dice [0.8221, 0.8022, 0.8516, 0.6901]\n2024-10-16 07:48:16.083454: Epoch time: 144.64 s\n2024-10-16 07:48:18.380716: \n2024-10-16 07:48:18.381024: Epoch 109\n2024-10-16 07:48:18.381207: Current learning rate: 0.00901\n2024-10-16 07:50:42.787284: train_loss -0.6333\n2024-10-16 07:50:42.787664: val_loss -0.6781\n2024-10-16 07:50:42.787843: Pseudo dice [0.8212, 0.8138, 0.8539, 0.7854]\n2024-10-16 07:50:42.787990: Epoch time: 144.41 s\n2024-10-16 07:50:45.099227: \n2024-10-16 07:50:45.099498: Epoch 110\n2024-10-16 07:50:45.099684: Current learning rate: 0.009\n2024-10-16 07:53:09.813032: train_loss -0.6433\n2024-10-16 07:53:09.813411: val_loss -0.6592\n2024-10-16 07:53:09.813638: Pseudo dice [0.8366, 0.7932, 0.8533, 0.6607]\n2024-10-16 07:53:09.813826: Epoch time: 144.72 s\n2024-10-16 07:53:12.284934: \n2024-10-16 07:53:12.285289: Epoch 111\n2024-10-16 07:53:12.285472: Current learning rate: 0.009\n2024-10-16 07:55:37.048568: train_loss -0.6212\n2024-10-16 07:55:37.048991: val_loss -0.6701\n2024-10-16 07:55:37.049217: Pseudo dice [0.8338, 0.7913, 0.8713, 0.6238]\n2024-10-16 07:55:37.049448: Epoch time: 144.77 s\n2024-10-16 07:55:39.428726: \n2024-10-16 07:55:39.429040: Epoch 112\n2024-10-16 07:55:39.429258: Current learning rate: 0.00899\n2024-10-16 07:58:04.043931: train_loss -0.6505\n2024-10-16 07:58:04.044355: val_loss -0.6457\n2024-10-16 07:58:04.044559: Pseudo dice [0.8226, 0.7837, 0.8394, 0.7797]\n2024-10-16 07:58:04.044719: Epoch time: 144.62 s\n2024-10-16 07:58:06.613473: \n2024-10-16 07:58:06.613800: Epoch 113\n2024-10-16 07:58:06.613971: Current learning rate: 0.00898\n2024-10-16 08:00:31.186932: train_loss -0.6493\n2024-10-16 08:00:31.187231: val_loss -0.6444\n2024-10-16 08:00:31.187424: Pseudo dice [0.8209, 0.8108, 0.8667, 0.6721]\n2024-10-16 08:00:31.187590: Epoch time: 144.58 s\n2024-10-16 08:00:33.495480: \n2024-10-16 08:00:33.495733: Epoch 114\n2024-10-16 08:00:33.495891: Current learning rate: 0.00897\n2024-10-16 08:02:58.051700: train_loss -0.6577\n2024-10-16 08:02:58.051970: val_loss -0.6919\n2024-10-16 08:02:58.052187: Pseudo dice [0.8383, 0.8338, 0.8674, 0.8302]\n2024-10-16 08:02:58.052330: Epoch time: 144.56 s\n2024-10-16 08:03:00.456150: \n2024-10-16 08:03:00.456485: Epoch 115\n2024-10-16 08:03:00.456630: Current learning rate: 0.00896\n2024-10-16 08:05:24.971998: train_loss -0.6785\n2024-10-16 08:05:24.972294: val_loss -0.6635\n2024-10-16 08:05:24.972517: Pseudo dice [0.8195, 0.8028, 0.857, 0.7317]\n2024-10-16 08:05:24.972665: Epoch time: 144.52 s\n2024-10-16 08:05:27.317949: \n2024-10-16 08:05:27.318328: Epoch 116\n2024-10-16 08:05:27.318474: Current learning rate: 0.00895\n2024-10-16 08:07:51.803956: train_loss -0.6444\n2024-10-16 08:07:51.804246: val_loss -0.6623\n2024-10-16 08:07:51.804435: Pseudo dice [0.8429, 0.8372, 0.8499, 0.8268]\n2024-10-16 08:07:51.804630: Epoch time: 144.49 s\n2024-10-16 08:07:51.804761: Yayy! New best EMA pseudo Dice: 0.8031\n2024-10-16 08:07:55.059205: \n2024-10-16 08:07:55.059542: Epoch 117\n2024-10-16 08:07:55.059713: Current learning rate: 0.00894\n2024-10-16 08:10:19.582280: train_loss -0.6462\n2024-10-16 08:10:19.582554: val_loss -0.6773\n2024-10-16 08:10:19.582797: Pseudo dice [0.8349, 0.8191, 0.8731, 0.6764]\n2024-10-16 08:10:19.582995: Epoch time: 144.53 s\n2024-10-16 08:10:21.924883: \n2024-10-16 08:10:21.925222: Epoch 118\n2024-10-16 08:10:21.925447: Current learning rate: 0.00893\n2024-10-16 08:12:46.498789: train_loss -0.6433\n2024-10-16 08:12:46.499115: val_loss -0.6652\n2024-10-16 08:12:46.499269: Pseudo dice [0.8224, 0.8067, 0.8645, 0.7642]\n2024-10-16 08:12:46.499389: Epoch time: 144.58 s\n2024-10-16 08:12:46.499483: Yayy! New best EMA pseudo Dice: 0.804\n2024-10-16 08:12:49.770862: \n2024-10-16 08:12:49.771217: Epoch 119\n2024-10-16 08:12:49.771400: Current learning rate: 0.00892\n2024-10-16 08:15:14.522651: train_loss -0.6431\n2024-10-16 08:15:14.522921: val_loss -0.6592\n2024-10-16 08:15:14.523146: Pseudo dice [0.7976, 0.7924, 0.8355, 0.7712]\n2024-10-16 08:15:14.523370: Epoch time: 144.75 s\n2024-10-16 08:15:16.909235: \n2024-10-16 08:15:16.909432: Epoch 120\n2024-10-16 08:15:16.909615: Current learning rate: 0.00891\n2024-10-16 08:17:41.902532: train_loss -0.6441\n2024-10-16 08:17:41.902840: val_loss -0.6679\n2024-10-16 08:17:41.902986: Pseudo dice [0.8249, 0.8107, 0.8665, 0.8365]\n2024-10-16 08:17:41.903203: Epoch time: 145.0 s\n2024-10-16 08:17:41.903351: Yayy! New best EMA pseudo Dice: 0.8066\n2024-10-16 08:17:46.295930: \n2024-10-16 08:17:46.296408: Epoch 121\n2024-10-16 08:17:46.296615: Current learning rate: 0.0089\n2024-10-16 08:20:11.624526: train_loss -0.6429\n2024-10-16 08:20:11.624802: val_loss -0.6119\n2024-10-16 08:20:11.625139: Pseudo dice [0.7615, 0.8037, 0.7811, 0.6796]\n2024-10-16 08:20:11.625411: Epoch time: 145.33 s\n2024-10-16 08:20:14.021332: \n2024-10-16 08:20:14.021735: Epoch 122\n2024-10-16 08:20:14.021928: Current learning rate: 0.00889\n2024-10-16 08:22:39.242727: train_loss -0.6219\n2024-10-16 08:22:39.243203: val_loss -0.6745\n2024-10-16 08:22:39.243454: Pseudo dice [0.8283, 0.8166, 0.8424, 0.8299]\n2024-10-16 08:22:39.243670: Epoch time: 145.22 s\n2024-10-16 08:22:41.860491: \n2024-10-16 08:22:41.860946: Epoch 123\n2024-10-16 08:22:41.861168: Current learning rate: 0.00889\n2024-10-16 08:25:07.161647: train_loss -0.6314\n2024-10-16 08:25:07.162106: val_loss -0.6593\n2024-10-16 08:25:07.162356: Pseudo dice [0.838, 0.8118, 0.8366, 0.758]\n2024-10-16 08:25:07.162511: Epoch time: 145.3 s\n2024-10-16 08:25:09.549403: \n2024-10-16 08:25:09.549749: Epoch 124\n2024-10-16 08:25:09.549891: Current learning rate: 0.00888\n2024-10-16 08:27:34.611708: train_loss -0.6502\n2024-10-16 08:27:34.612027: val_loss -0.6691\n2024-10-16 08:27:34.612287: Pseudo dice [0.8293, 0.8231, 0.8489, 0.7153]\n2024-10-16 08:27:34.612562: Epoch time: 145.07 s\n2024-10-16 08:27:37.042653: \n2024-10-16 08:27:37.042994: Epoch 125\n2024-10-16 08:27:37.043207: Current learning rate: 0.00887\n2024-10-16 08:30:02.216003: train_loss -0.6507\n2024-10-16 08:30:02.216323: val_loss -0.6655\n2024-10-16 08:30:02.216559: Pseudo dice [0.8202, 0.8276, 0.8289, 0.8243]\n2024-10-16 08:30:02.216716: Epoch time: 145.18 s\n2024-10-16 08:30:02.216836: Yayy! New best EMA pseudo Dice: 0.807\n2024-10-16 08:30:05.281072: \n2024-10-16 08:30:05.281458: Epoch 126\n2024-10-16 08:30:05.281606: Current learning rate: 0.00886\n2024-10-16 08:32:30.124504: train_loss -0.6442\n2024-10-16 08:32:30.124765: val_loss -0.6691\n2024-10-16 08:32:30.125035: Pseudo dice [0.8286, 0.8052, 0.849, 0.808]\n2024-10-16 08:32:30.125220: Epoch time: 144.85 s\n2024-10-16 08:32:30.125362: Yayy! New best EMA pseudo Dice: 0.8086\n2024-10-16 08:32:33.263482: \n2024-10-16 08:32:33.263721: Epoch 127\n2024-10-16 08:32:33.263958: Current learning rate: 0.00885\n2024-10-16 08:34:58.341459: train_loss -0.6226\n2024-10-16 08:34:58.341749: val_loss -0.661\n2024-10-16 08:34:58.341918: Pseudo dice [0.8225, 0.8122, 0.8656, 0.7486]\n2024-10-16 08:34:58.342183: Epoch time: 145.08 s\n2024-10-16 08:34:58.342294: Yayy! New best EMA pseudo Dice: 0.8089\n2024-10-16 08:35:01.499114: \n2024-10-16 08:35:01.499476: Epoch 128\n2024-10-16 08:35:01.499658: Current learning rate: 0.00884\n2024-10-16 08:37:26.616881: train_loss -0.6422\n2024-10-16 08:37:26.617182: val_loss -0.6634\n2024-10-16 08:37:26.617339: Pseudo dice [0.8182, 0.7965, 0.8638, 0.7221]\n2024-10-16 08:37:26.617545: Epoch time: 145.12 s\n2024-10-16 08:37:29.012681: \n2024-10-16 08:37:29.013036: Epoch 129\n2024-10-16 08:37:29.013241: Current learning rate: 0.00883\n2024-10-16 08:39:53.633508: train_loss -0.6571\n2024-10-16 08:39:53.633770: val_loss -0.6714\n2024-10-16 08:39:53.633950: Pseudo dice [0.8152, 0.8138, 0.8545, 0.8014]\n2024-10-16 08:39:53.634084: Epoch time: 144.62 s\n2024-10-16 08:39:53.634232: Yayy! New best EMA pseudo Dice: 0.8094\n2024-10-16 08:39:56.784278: \n2024-10-16 08:39:56.784659: Epoch 130\n2024-10-16 08:39:56.784858: Current learning rate: 0.00882\n2024-10-16 08:42:21.316195: train_loss -0.63\n2024-10-16 08:42:21.316586: val_loss -0.6605\n2024-10-16 08:42:21.316776: Pseudo dice [0.8433, 0.8038, 0.796, 0.668]\n2024-10-16 08:42:21.316900: Epoch time: 144.53 s\n2024-10-16 08:42:23.685735: \n2024-10-16 08:42:23.686005: Epoch 131\n2024-10-16 08:42:23.686178: Current learning rate: 0.00881\n2024-10-16 08:44:48.559065: train_loss -0.6643\n2024-10-16 08:44:48.559355: val_loss -0.6701\n2024-10-16 08:44:48.559662: Pseudo dice [0.8216, 0.8153, 0.8693, 0.7055]\n2024-10-16 08:44:48.559801: Epoch time: 144.88 s\n2024-10-16 08:44:50.876447: \n2024-10-16 08:44:50.876666: Epoch 132\n2024-10-16 08:44:50.876828: Current learning rate: 0.0088\n2024-10-16 08:47:15.287605: train_loss -0.6664\n2024-10-16 08:47:15.287865: val_loss -0.6716\n2024-10-16 08:47:15.288062: Pseudo dice [0.831, 0.8168, 0.8573, 0.7869]\n2024-10-16 08:47:15.288277: Epoch time: 144.41 s\n2024-10-16 08:47:17.708714: \n2024-10-16 08:47:17.708971: Epoch 133\n2024-10-16 08:47:17.709174: Current learning rate: 0.00879\n2024-10-16 08:49:42.215381: train_loss -0.6747\n2024-10-16 08:49:42.215685: val_loss -0.6581\n2024-10-16 08:49:42.215958: Pseudo dice [0.8225, 0.8408, 0.8477, 0.7479]\n2024-10-16 08:49:42.216081: Epoch time: 144.51 s\n2024-10-16 08:49:44.566233: \n2024-10-16 08:49:44.566565: Epoch 134\n2024-10-16 08:49:44.566715: Current learning rate: 0.00879\n2024-10-16 08:52:09.037143: train_loss -0.6474\n2024-10-16 08:52:09.037506: val_loss -0.6786\n2024-10-16 08:52:09.037734: Pseudo dice [0.8291, 0.8245, 0.8538, 0.7134]\n2024-10-16 08:52:09.038025: Epoch time: 144.47 s\n2024-10-16 08:52:11.458760: \n2024-10-16 08:52:11.459064: Epoch 135\n2024-10-16 08:52:11.459240: Current learning rate: 0.00878\n2024-10-16 08:54:35.785710: train_loss -0.6622\n2024-10-16 08:54:35.786137: val_loss -0.6595\n2024-10-16 08:54:35.786318: Pseudo dice [0.8339, 0.804, 0.8556, 0.7054]\n2024-10-16 08:54:35.786472: Epoch time: 144.33 s\n2024-10-16 08:54:38.256944: \n2024-10-16 08:54:38.257379: Epoch 136\n2024-10-16 08:54:38.257582: Current learning rate: 0.00877\n2024-10-16 08:57:02.734968: train_loss -0.656\n2024-10-16 08:57:02.735304: val_loss -0.6859\n2024-10-16 08:57:02.735445: Pseudo dice [0.8495, 0.8363, 0.8603, 0.5653]\n2024-10-16 08:57:02.735584: Epoch time: 144.48 s\n2024-10-16 08:57:05.124733: \n2024-10-16 08:57:05.124960: Epoch 137\n2024-10-16 08:57:05.125166: Current learning rate: 0.00876\n2024-10-16 08:59:29.801820: train_loss -0.6731\n2024-10-16 08:59:29.802055: val_loss -0.6774\n2024-10-16 08:59:29.802226: Pseudo dice [0.8376, 0.82, 0.8688, 0.789]\n2024-10-16 08:59:29.802340: Epoch time: 144.68 s\n2024-10-16 08:59:32.191854: \n2024-10-16 08:59:32.192193: Epoch 138\n2024-10-16 08:59:32.192391: Current learning rate: 0.00875\n2024-10-16 09:01:57.120592: train_loss -0.6586\n2024-10-16 09:01:57.120946: val_loss -0.6832\n2024-10-16 09:01:57.121157: Pseudo dice [0.8341, 0.8073, 0.8734, 0.8134]\n2024-10-16 09:01:57.121317: Epoch time: 144.93 s\n2024-10-16 09:01:59.802562: \n2024-10-16 09:01:59.803012: Epoch 139\n2024-10-16 09:01:59.803257: Current learning rate: 0.00874\n2024-10-16 09:04:24.828632: train_loss -0.6496\n2024-10-16 09:04:24.828935: val_loss -0.6533\n2024-10-16 09:04:24.829208: Pseudo dice [0.8312, 0.7865, 0.8589, 0.8019]\n2024-10-16 09:04:24.829422: Epoch time: 145.03 s\n2024-10-16 09:04:24.829601: Yayy! New best EMA pseudo Dice: 0.8103\n2024-10-16 09:04:28.692487: \n2024-10-16 09:04:28.692940: Epoch 140\n2024-10-16 09:04:28.693184: Current learning rate: 0.00873\n2024-10-16 09:06:54.056721: train_loss -0.6389\n2024-10-16 09:06:54.057041: val_loss -0.6545\n2024-10-16 09:06:54.057520: Pseudo dice [0.8317, 0.829, 0.8157, 0.7396]\n2024-10-16 09:06:54.057714: Epoch time: 145.37 s\n2024-10-16 09:06:58.157690: \n2024-10-16 09:06:58.158138: Epoch 141\n2024-10-16 09:06:58.158387: Current learning rate: 0.00872\n2024-10-16 09:09:23.368518: train_loss -0.6687\n2024-10-16 09:09:23.368854: val_loss -0.7011\n2024-10-16 09:09:23.369065: Pseudo dice [0.8168, 0.8148, 0.8679, 0.7963]\n2024-10-16 09:09:23.369293: Epoch time: 145.21 s\n2024-10-16 09:09:23.369470: Yayy! New best EMA pseudo Dice: 0.8111\n2024-10-16 09:09:27.213594: \n2024-10-16 09:09:27.214031: Epoch 142\n2024-10-16 09:09:27.214269: Current learning rate: 0.00871\n2024-10-16 09:11:52.468219: train_loss -0.6683\n2024-10-16 09:11:52.468508: val_loss -0.6599\n2024-10-16 09:11:52.468707: Pseudo dice [0.8194, 0.835, 0.8578, 0.7898]\n2024-10-16 09:11:52.468844: Epoch time: 145.26 s\n2024-10-16 09:11:52.468959: Yayy! New best EMA pseudo Dice: 0.8125\n2024-10-16 09:11:55.951902: \n2024-10-16 09:11:55.952322: Epoch 143\n2024-10-16 09:11:55.952485: Current learning rate: 0.0087\n2024-10-16 09:14:21.083366: train_loss -0.6596\n2024-10-16 09:14:21.083799: val_loss -0.6655\n2024-10-16 09:14:21.083950: Pseudo dice [0.8345, 0.7955, 0.8717, 0.7662]\n2024-10-16 09:14:21.084059: Epoch time: 145.13 s\n2024-10-16 09:14:21.084185: Yayy! New best EMA pseudo Dice: 0.813\n2024-10-16 09:14:24.481409: \n2024-10-16 09:14:24.481765: Epoch 144\n2024-10-16 09:14:24.481943: Current learning rate: 0.00869\n2024-10-16 09:16:49.797851: train_loss -0.6677\n2024-10-16 09:16:49.798208: val_loss -0.7058\n2024-10-16 09:16:49.798419: Pseudo dice [0.8352, 0.82, 0.891, 0.8195]\n2024-10-16 09:16:49.798531: Epoch time: 145.32 s\n2024-10-16 09:16:49.798618: Yayy! New best EMA pseudo Dice: 0.8158\n2024-10-16 09:16:53.138318: \n2024-10-16 09:16:53.138655: Epoch 145\n2024-10-16 09:16:53.138840: Current learning rate: 0.00868\n2024-10-16 09:19:18.622581: train_loss -0.6712\n2024-10-16 09:19:18.623009: val_loss -0.6706\n2024-10-16 09:19:18.623240: Pseudo dice [0.8183, 0.83, 0.8469, 0.7937]\n2024-10-16 09:19:18.623445: Epoch time: 145.49 s\n2024-10-16 09:19:18.623582: Yayy! New best EMA pseudo Dice: 0.8165\n2024-10-16 09:19:22.603972: \n2024-10-16 09:19:22.604516: Epoch 146\n2024-10-16 09:19:22.604741: Current learning rate: 0.00868\n2024-10-16 09:21:48.485837: train_loss -0.6719\n2024-10-16 09:21:48.486175: val_loss -0.7042\n2024-10-16 09:21:48.486345: Pseudo dice [0.8413, 0.8378, 0.8762, 0.7945]\n2024-10-16 09:21:48.486686: Epoch time: 145.89 s\n2024-10-16 09:21:48.486995: Yayy! New best EMA pseudo Dice: 0.8185\n2024-10-16 09:21:52.445716: \n2024-10-16 09:21:52.446405: Epoch 147\n2024-10-16 09:21:52.446639: Current learning rate: 0.00867\n2024-10-16 09:24:18.076590: train_loss -0.6733\n2024-10-16 09:24:18.076979: val_loss -0.672\n2024-10-16 09:24:18.077216: Pseudo dice [0.834, 0.8453, 0.8395, 0.81]\n2024-10-16 09:24:18.077394: Epoch time: 145.63 s\n2024-10-16 09:24:18.077538: Yayy! New best EMA pseudo Dice: 0.8199\n2024-10-16 09:24:22.083395: \n2024-10-16 09:24:22.083939: Epoch 148\n2024-10-16 09:24:22.084194: Current learning rate: 0.00866\n2024-10-16 09:26:47.647214: train_loss -0.6596\n2024-10-16 09:26:47.647706: val_loss -0.6474\n2024-10-16 09:26:47.647884: Pseudo dice [0.8295, 0.802, 0.8202, 0.8261]\n2024-10-16 09:26:47.648019: Epoch time: 145.57 s\n2024-10-16 09:26:50.662859: \n2024-10-16 09:26:50.663405: Epoch 149\n2024-10-16 09:26:50.663673: Current learning rate: 0.00865\n2024-10-16 09:29:16.288278: train_loss -0.6569\n2024-10-16 09:29:16.288826: val_loss -0.7041\n2024-10-16 09:29:16.289001: Pseudo dice [0.8422, 0.8205, 0.8675, 0.8232]\n2024-10-16 09:29:16.289168: Epoch time: 145.63 s\n2024-10-16 09:29:17.305900: Yayy! New best EMA pseudo Dice: 0.8217\n2024-10-16 09:29:21.350049: \n2024-10-16 09:29:21.350685: Epoch 150\n2024-10-16 09:29:21.350961: Current learning rate: 0.00864\n2024-10-16 09:31:47.234539: train_loss -0.6725\n2024-10-16 09:31:47.234897: val_loss -0.6798\n2024-10-16 09:31:47.235192: Pseudo dice [0.8477, 0.8089, 0.8878, 0.8279]\n2024-10-16 09:31:47.235380: Epoch time: 145.89 s\n2024-10-16 09:31:47.235548: Yayy! New best EMA pseudo Dice: 0.8239\n2024-10-16 09:31:51.334951: \n2024-10-16 09:31:51.335464: Epoch 151\n2024-10-16 09:31:51.335669: Current learning rate: 0.00863\n2024-10-16 09:34:16.932018: train_loss -0.663\n2024-10-16 09:34:16.932522: val_loss -0.6725\n2024-10-16 09:34:16.932696: Pseudo dice [0.8343, 0.8267, 0.8479, 0.7354]\n2024-10-16 09:34:16.932827: Epoch time: 145.6 s\n2024-10-16 09:34:19.894793: \n2024-10-16 09:34:19.895340: Epoch 152\n2024-10-16 09:34:19.895581: Current learning rate: 0.00862\n2024-10-16 09:36:45.518644: train_loss -0.6694\n2024-10-16 09:36:45.519056: val_loss -0.6955\n2024-10-16 09:36:45.519325: Pseudo dice [0.8532, 0.8453, 0.8783, 0.812]\n2024-10-16 09:36:45.519493: Epoch time: 145.63 s\n2024-10-16 09:36:45.519639: Yayy! New best EMA pseudo Dice: 0.825\n2024-10-16 09:36:49.509934: \n2024-10-16 09:36:49.510434: Epoch 153\n2024-10-16 09:36:49.510663: Current learning rate: 0.00861\n2024-10-16 09:39:15.001181: train_loss -0.6616\n2024-10-16 09:39:15.001513: val_loss -0.6719\n2024-10-16 09:39:15.001732: Pseudo dice [0.8275, 0.8073, 0.8666, 0.7875]\n2024-10-16 09:39:15.001895: Epoch time: 145.49 s\n2024-10-16 09:39:18.002447: \n2024-10-16 09:39:18.002908: Epoch 154\n2024-10-16 09:39:18.003157: Current learning rate: 0.0086\n2024-10-16 09:41:43.648062: train_loss -0.6691\n2024-10-16 09:41:43.648536: val_loss -0.6712\n2024-10-16 09:41:43.648724: Pseudo dice [0.853, 0.8256, 0.8865, 0.7594]\n2024-10-16 09:41:43.648861: Epoch time: 145.65 s\n2024-10-16 09:41:43.648966: Yayy! New best EMA pseudo Dice: 0.8254\n2024-10-16 09:41:47.694947: \n2024-10-16 09:41:47.695458: Epoch 155\n2024-10-16 09:41:47.695699: Current learning rate: 0.00859\n2024-10-16 09:44:13.404907: train_loss -0.6709\n2024-10-16 09:44:13.405256: val_loss -0.666\n2024-10-16 09:44:13.405433: Pseudo dice [0.8318, 0.8136, 0.882, 0.6055]\n2024-10-16 09:44:13.405659: Epoch time: 145.71 s\n2024-10-16 09:44:16.445145: \n2024-10-16 09:44:16.445659: Epoch 156\n2024-10-16 09:44:16.445862: Current learning rate: 0.00858\n2024-10-16 09:46:41.892719: train_loss -0.6585\n2024-10-16 09:46:41.893317: val_loss -0.6768\n2024-10-16 09:46:41.893569: Pseudo dice [0.8385, 0.8101, 0.8701, 0.7888]\n2024-10-16 09:46:41.893780: Epoch time: 145.45 s\n2024-10-16 09:46:44.861467: \n2024-10-16 09:46:44.861873: Epoch 157\n2024-10-16 09:46:44.862124: Current learning rate: 0.00858\n2024-10-16 09:49:10.439914: train_loss -0.6649\n2024-10-16 09:49:10.440316: val_loss -0.7157\n2024-10-16 09:49:10.440559: Pseudo dice [0.8427, 0.8314, 0.8771, 0.8027]\n2024-10-16 09:49:10.440766: Epoch time: 145.58 s\n2024-10-16 09:49:13.394842: \n2024-10-16 09:49:13.395515: Epoch 158\n2024-10-16 09:49:13.395766: Current learning rate: 0.00857\n2024-10-16 09:51:38.962575: train_loss -0.6447\n2024-10-16 09:51:38.962921: val_loss -0.6653\n2024-10-16 09:51:38.963321: Pseudo dice [0.8171, 0.8402, 0.833, 0.7763]\n2024-10-16 09:51:38.963507: Epoch time: 145.57 s\n2024-10-16 09:51:42.037011: \n2024-10-16 09:51:42.037418: Epoch 159\n2024-10-16 09:51:42.037646: Current learning rate: 0.00856\n2024-10-16 09:54:07.409168: train_loss -0.6434\n2024-10-16 09:54:07.409505: val_loss -0.659\n2024-10-16 09:54:07.409837: Pseudo dice [0.8348, 0.8154, 0.8625, 0.7908]\n2024-10-16 09:54:07.410059: Epoch time: 145.38 s\n2024-10-16 09:54:10.383220: \n2024-10-16 09:54:10.383640: Epoch 160\n2024-10-16 09:54:10.383820: Current learning rate: 0.00855\n2024-10-16 09:56:36.117865: train_loss -0.6711\n2024-10-16 09:56:36.118383: val_loss -0.6387\n2024-10-16 09:56:36.118671: Pseudo dice [0.822, 0.8032, 0.8728, 0.7295]\n2024-10-16 09:56:36.118850: Epoch time: 145.74 s\n2024-10-16 09:56:39.847863: \n2024-10-16 09:56:39.848187: Epoch 161\n2024-10-16 09:56:39.848361: Current learning rate: 0.00854\n2024-10-16 09:59:04.905199: train_loss -0.6629\n2024-10-16 09:59:04.905650: val_loss -0.6696\n2024-10-16 09:59:04.905857: Pseudo dice [0.8387, 0.8294, 0.878, 0.7034]\n2024-10-16 09:59:04.906029: Epoch time: 145.06 s\n2024-10-16 09:59:07.949795: \n2024-10-16 09:59:07.950280: Epoch 162\n2024-10-16 09:59:07.950501: Current learning rate: 0.00853\n2024-10-16 10:01:33.240503: train_loss -0.652\n2024-10-16 10:01:33.241019: val_loss -0.6802\n2024-10-16 10:01:33.241281: Pseudo dice [0.821, 0.8045, 0.8621, 0.7996]\n2024-10-16 10:01:33.241426: Epoch time: 145.29 s\n2024-10-16 10:01:36.145501: \n2024-10-16 10:01:36.145976: Epoch 163\n2024-10-16 10:01:36.146197: Current learning rate: 0.00852\n2024-10-16 10:04:01.115973: train_loss -0.6792\n2024-10-16 10:04:01.116353: val_loss -0.666\n2024-10-16 10:04:01.116583: Pseudo dice [0.8062, 0.8235, 0.8657, 0.8059]\n2024-10-16 10:04:01.116875: Epoch time: 144.97 s\n2024-10-16 10:04:04.011928: \n2024-10-16 10:04:04.012381: Epoch 164\n2024-10-16 10:04:04.012619: Current learning rate: 0.00851\n2024-10-16 10:06:29.157878: train_loss -0.6319\n2024-10-16 10:06:29.158409: val_loss -0.6443\n2024-10-16 10:06:29.158581: Pseudo dice [0.8169, 0.8008, 0.8559, 0.8012]\n2024-10-16 10:06:29.158708: Epoch time: 145.15 s\n2024-10-16 10:06:31.924995: \n2024-10-16 10:06:31.925426: Epoch 165\n2024-10-16 10:06:31.925640: Current learning rate: 0.0085\n2024-10-16 10:08:56.990782: train_loss -0.6432\n2024-10-16 10:08:56.991082: val_loss -0.6753\n2024-10-16 10:08:56.991336: Pseudo dice [0.827, 0.8246, 0.8386, 0.7401]\n2024-10-16 10:08:56.991495: Epoch time: 145.07 s\n2024-10-16 10:08:59.541811: \n2024-10-16 10:08:59.542195: Epoch 166\n2024-10-16 10:08:59.542384: Current learning rate: 0.00849\n2024-10-16 10:11:24.479716: train_loss -0.654\n2024-10-16 10:11:24.480030: val_loss -0.6907\n2024-10-16 10:11:24.480276: Pseudo dice [0.8432, 0.8289, 0.8602, 0.8284]\n2024-10-16 10:11:24.480561: Epoch time: 144.94 s\n2024-10-16 10:11:27.332879: \n2024-10-16 10:11:27.333393: Epoch 167\n2024-10-16 10:11:27.333623: Current learning rate: 0.00848\n2024-10-16 10:13:51.991622: train_loss -0.664\n2024-10-16 10:13:51.992157: val_loss -0.6816\n2024-10-16 10:13:51.992344: Pseudo dice [0.8423, 0.8191, 0.8821, 0.8056]\n2024-10-16 10:13:51.992482: Epoch time: 144.66 s\n2024-10-16 10:13:54.506457: \n2024-10-16 10:13:54.506879: Epoch 168\n2024-10-16 10:13:54.507068: Current learning rate: 0.00847\n2024-10-16 10:16:19.155127: train_loss -0.6577\n2024-10-16 10:16:19.155385: val_loss -0.6782\n2024-10-16 10:16:19.155553: Pseudo dice [0.8334, 0.8401, 0.8507, 0.8302]\n2024-10-16 10:16:19.155684: Epoch time: 144.65 s\n2024-10-16 10:16:21.511435: \n2024-10-16 10:16:21.511750: Epoch 169\n2024-10-16 10:16:21.511970: Current learning rate: 0.00847\n2024-10-16 10:18:46.513042: train_loss -0.6506\n2024-10-16 10:18:46.513479: val_loss -0.6831\n2024-10-16 10:18:46.513713: Pseudo dice [0.8225, 0.816, 0.8643, 0.8338]\n2024-10-16 10:18:46.513913: Epoch time: 145.0 s\n2024-10-16 10:18:46.514054: Yayy! New best EMA pseudo Dice: 0.8257\n2024-10-16 10:18:50.574989: \n2024-10-16 10:18:50.575410: Epoch 170\n2024-10-16 10:18:50.575616: Current learning rate: 0.00846\n2024-10-16 10:21:15.113790: train_loss -0.663\n2024-10-16 10:21:15.114316: val_loss -0.6831\n2024-10-16 10:21:15.114518: Pseudo dice [0.8229, 0.8269, 0.8528, 0.8004]\n2024-10-16 10:21:15.114681: Epoch time: 144.54 s\n2024-10-16 10:21:15.114825: Yayy! New best EMA pseudo Dice: 0.8257\n2024-10-16 10:21:18.229685: \n2024-10-16 10:21:18.229941: Epoch 171\n2024-10-16 10:21:18.230134: Current learning rate: 0.00845\n2024-10-16 10:23:43.023710: train_loss -0.6865\n2024-10-16 10:23:43.024130: val_loss -0.6781\n2024-10-16 10:23:43.024360: Pseudo dice [0.8391, 0.8177, 0.8595, 0.8107]\n2024-10-16 10:23:43.024552: Epoch time: 144.8 s\n2024-10-16 10:23:43.024732: Yayy! New best EMA pseudo Dice: 0.8263\n2024-10-16 10:23:46.649154: \n2024-10-16 10:23:46.649497: Epoch 172\n2024-10-16 10:23:46.649687: Current learning rate: 0.00844\n2024-10-16 10:26:11.775039: train_loss -0.6667\n2024-10-16 10:26:11.775343: val_loss -0.6652\n2024-10-16 10:26:11.775520: Pseudo dice [0.8113, 0.8175, 0.8369, 0.8262]\n2024-10-16 10:26:11.775654: Epoch time: 145.13 s\n2024-10-16 10:26:14.173831: \n2024-10-16 10:26:14.174180: Epoch 173\n2024-10-16 10:26:14.174379: Current learning rate: 0.00843\n2024-10-16 10:28:39.155843: train_loss -0.6657\n2024-10-16 10:28:39.156132: val_loss -0.6844\n2024-10-16 10:28:39.156301: Pseudo dice [0.8396, 0.8274, 0.8733, 0.6696]\n2024-10-16 10:28:39.156441: Epoch time: 144.99 s\n2024-10-16 10:28:41.548778: \n2024-10-16 10:28:41.549074: Epoch 174\n2024-10-16 10:28:41.549264: Current learning rate: 0.00842\n2024-10-16 10:31:06.584498: train_loss -0.6623\n2024-10-16 10:31:06.584893: val_loss -0.6849\n2024-10-16 10:31:06.585078: Pseudo dice [0.8361, 0.8178, 0.8764, 0.857]\n2024-10-16 10:31:06.585256: Epoch time: 145.04 s\n2024-10-16 10:31:08.951774: \n2024-10-16 10:31:08.951962: Epoch 175\n2024-10-16 10:31:08.952161: Current learning rate: 0.00841\n2024-10-16 10:33:33.782764: train_loss -0.6723\n2024-10-16 10:33:33.783052: val_loss -0.707\n2024-10-16 10:33:33.783321: Pseudo dice [0.8379, 0.8201, 0.8632, 0.8094]\n2024-10-16 10:33:33.783613: Epoch time: 144.83 s\n2024-10-16 10:33:33.783874: Yayy! New best EMA pseudo Dice: 0.8266\n2024-10-16 10:33:37.161816: \n2024-10-16 10:33:37.162035: Epoch 176\n2024-10-16 10:33:37.162245: Current learning rate: 0.0084\n2024-10-16 10:36:02.006401: train_loss -0.6668\n2024-10-16 10:36:02.006698: val_loss -0.6815\n2024-10-16 10:36:02.006871: Pseudo dice [0.8243, 0.8154, 0.8714, 0.6764]\n2024-10-16 10:36:02.007005: Epoch time: 144.85 s\n2024-10-16 10:36:04.396271: \n2024-10-16 10:36:04.396531: Epoch 177\n2024-10-16 10:36:04.396687: Current learning rate: 0.00839\n2024-10-16 10:38:29.014666: train_loss -0.6509\n2024-10-16 10:38:29.015141: val_loss -0.7024\n2024-10-16 10:38:29.015329: Pseudo dice [0.8328, 0.8331, 0.8681, 0.7943]\n2024-10-16 10:38:29.015510: Epoch time: 144.62 s\n2024-10-16 10:38:31.393592: \n2024-10-16 10:38:31.393866: Epoch 178\n2024-10-16 10:38:31.394050: Current learning rate: 0.00838\n2024-10-16 10:40:55.956918: train_loss -0.6593\n2024-10-16 10:40:55.957369: val_loss -0.6766\n2024-10-16 10:40:55.957516: Pseudo dice [0.8295, 0.8099, 0.8667, 0.7706]\n2024-10-16 10:40:55.957635: Epoch time: 144.57 s\n2024-10-16 10:40:58.428789: \n2024-10-16 10:40:58.429073: Epoch 179\n2024-10-16 10:40:58.429264: Current learning rate: 0.00837\n2024-10-16 10:43:23.220309: train_loss -0.6559\n2024-10-16 10:43:23.220749: val_loss -0.6347\n2024-10-16 10:43:23.220933: Pseudo dice [0.838, 0.7838, 0.83, 0.627]\n2024-10-16 10:43:23.221062: Epoch time: 144.79 s\n2024-10-16 10:43:26.584035: \n2024-10-16 10:43:26.584426: Epoch 180\n2024-10-16 10:43:26.584625: Current learning rate: 0.00836\n2024-10-16 10:45:51.178814: train_loss -0.6787\n2024-10-16 10:45:51.179081: val_loss -0.678\n2024-10-16 10:45:51.179308: Pseudo dice [0.8254, 0.801, 0.8568, 0.7362]\n2024-10-16 10:45:51.179470: Epoch time: 144.6 s\n2024-10-16 10:45:53.490693: \n2024-10-16 10:45:53.491030: Epoch 181\n2024-10-16 10:45:53.491224: Current learning rate: 0.00836\n2024-10-16 10:48:17.954431: train_loss -0.6526\n2024-10-16 10:48:17.954882: val_loss -0.6746\n2024-10-16 10:48:17.955064: Pseudo dice [0.8253, 0.8253, 0.842, 0.685]\n2024-10-16 10:48:17.955248: Epoch time: 144.47 s\n2024-10-16 10:48:20.351608: \n2024-10-16 10:48:20.351995: Epoch 182\n2024-10-16 10:48:20.352211: Current learning rate: 0.00835\n2024-10-16 10:50:44.909245: train_loss -0.67\n2024-10-16 10:50:44.909658: val_loss -0.6883\n2024-10-16 10:50:44.909793: Pseudo dice [0.8444, 0.8275, 0.8835, 0.6689]\n2024-10-16 10:50:44.909915: Epoch time: 144.56 s\n2024-10-16 10:50:47.226365: \n2024-10-16 10:50:47.226655: Epoch 183\n2024-10-16 10:50:47.226823: Current learning rate: 0.00834\n2024-10-16 10:53:11.916712: train_loss -0.6598\n2024-10-16 10:53:11.917054: val_loss -0.6773\n2024-10-16 10:53:11.917248: Pseudo dice [0.8449, 0.8303, 0.8774, 0.7669]\n2024-10-16 10:53:11.917381: Epoch time: 144.69 s\n2024-10-16 10:53:14.247657: \n2024-10-16 10:53:14.247934: Epoch 184\n2024-10-16 10:53:14.248139: Current learning rate: 0.00833\n2024-10-16 10:55:39.071492: train_loss -0.6731\n2024-10-16 10:55:39.071766: val_loss -0.6873\n2024-10-16 10:55:39.071921: Pseudo dice [0.8431, 0.8261, 0.8803, 0.7083]\n2024-10-16 10:55:39.072127: Epoch time: 144.83 s\n2024-10-16 10:55:41.400961: \n2024-10-16 10:55:41.401212: Epoch 185\n2024-10-16 10:55:41.401358: Current learning rate: 0.00832\n2024-10-16 10:58:06.180527: train_loss -0.6785\n2024-10-16 10:58:06.180789: val_loss -0.6986\n2024-10-16 10:58:06.180971: Pseudo dice [0.8553, 0.8356, 0.8558, 0.7564]\n2024-10-16 10:58:06.181194: Epoch time: 144.78 s\n2024-10-16 10:58:08.718635: \n2024-10-16 10:58:08.718911: Epoch 186\n2024-10-16 10:58:08.719055: Current learning rate: 0.00831\n2024-10-16 11:00:33.152667: train_loss -0.6744\n2024-10-16 11:00:33.152937: val_loss -0.7197\n2024-10-16 11:00:33.153174: Pseudo dice [0.8365, 0.833, 0.8864, 0.7683]\n2024-10-16 11:00:33.153313: Epoch time: 144.44 s\n2024-10-16 11:00:35.538043: \n2024-10-16 11:00:35.538350: Epoch 187\n2024-10-16 11:00:35.538530: Current learning rate: 0.0083\n2024-10-16 11:02:59.984544: train_loss -0.6838\n2024-10-16 11:02:59.984861: val_loss -0.7054\n2024-10-16 11:02:59.985047: Pseudo dice [0.8442, 0.8411, 0.8722, 0.8443]\n2024-10-16 11:02:59.985240: Epoch time: 144.45 s\n2024-10-16 11:03:02.401024: \n2024-10-16 11:03:02.401275: Epoch 188\n2024-10-16 11:03:02.401430: Current learning rate: 0.00829\n2024-10-16 11:05:26.987199: train_loss -0.6872\n2024-10-16 11:05:26.987621: val_loss -0.7119\n2024-10-16 11:05:26.987956: Pseudo dice [0.8526, 0.8423, 0.8821, 0.8169]\n2024-10-16 11:05:26.988197: Epoch time: 144.59 s\n2024-10-16 11:05:29.335875: \n2024-10-16 11:05:29.336264: Epoch 189\n2024-10-16 11:05:29.336464: Current learning rate: 0.00828\n2024-10-16 11:07:53.872779: train_loss -0.6702\n2024-10-16 11:07:53.873103: val_loss -0.7181\n2024-10-16 11:07:53.873275: Pseudo dice [0.8403, 0.8079, 0.8898, 0.8197]\n2024-10-16 11:07:53.873403: Epoch time: 144.54 s\n2024-10-16 11:07:56.261120: \n2024-10-16 11:07:56.261445: Epoch 190\n2024-10-16 11:07:56.261633: Current learning rate: 0.00827\n2024-10-16 11:10:20.896679: train_loss -0.676\n2024-10-16 11:10:20.896965: val_loss -0.7076\n2024-10-16 11:10:20.897167: Pseudo dice [0.8569, 0.8552, 0.8948, 0.7812]\n2024-10-16 11:10:20.897324: Epoch time: 144.64 s\n2024-10-16 11:10:20.897427: Yayy! New best EMA pseudo Dice: 0.8276\n2024-10-16 11:10:24.134564: \n2024-10-16 11:10:24.134903: Epoch 191\n2024-10-16 11:10:24.135103: Current learning rate: 0.00826\n2024-10-16 11:12:48.637463: train_loss -0.6745\n2024-10-16 11:12:48.637873: val_loss -0.6972\n2024-10-16 11:12:48.638063: Pseudo dice [0.8461, 0.8434, 0.8866, 0.6854]\n2024-10-16 11:12:48.638248: Epoch time: 144.51 s\n2024-10-16 11:12:51.086994: \n2024-10-16 11:12:51.087420: Epoch 192\n2024-10-16 11:12:51.087621: Current learning rate: 0.00825\n2024-10-16 11:15:15.584303: train_loss -0.671\n2024-10-16 11:15:15.584725: val_loss -0.6743\n2024-10-16 11:15:15.584925: Pseudo dice [0.826, 0.8213, 0.8724, 0.8073]\n2024-10-16 11:15:15.585031: Epoch time: 144.5 s\n2024-10-16 11:15:18.217262: \n2024-10-16 11:15:18.217545: Epoch 193\n2024-10-16 11:15:18.217718: Current learning rate: 0.00824\n2024-10-16 11:17:42.787303: train_loss -0.6929\n2024-10-16 11:17:42.787595: val_loss -0.6939\n2024-10-16 11:17:42.787816: Pseudo dice [0.8466, 0.8312, 0.881, 0.7289]\n2024-10-16 11:17:42.788059: Epoch time: 144.57 s\n2024-10-16 11:17:45.200866: \n2024-10-16 11:17:45.201140: Epoch 194\n2024-10-16 11:17:45.201319: Current learning rate: 0.00824\n2024-10-16 11:20:10.023359: train_loss -0.6813\n2024-10-16 11:20:10.023687: val_loss -0.7256\n2024-10-16 11:20:10.023835: Pseudo dice [0.8523, 0.8541, 0.8938, 0.8393]\n2024-10-16 11:20:10.024034: Epoch time: 144.83 s\n2024-10-16 11:20:10.024211: Yayy! New best EMA pseudo Dice: 0.8298\n2024-10-16 11:20:13.349170: \n2024-10-16 11:20:13.349521: Epoch 195\n2024-10-16 11:20:13.349718: Current learning rate: 0.00823\n2024-10-16 11:22:38.384179: train_loss -0.6891\n2024-10-16 11:22:38.384646: val_loss -0.68\n2024-10-16 11:22:38.384887: Pseudo dice [0.8444, 0.8323, 0.8711, 0.7702]\n2024-10-16 11:22:38.385063: Epoch time: 145.04 s\n2024-10-16 11:22:40.936545: \n2024-10-16 11:22:40.936863: Epoch 196\n2024-10-16 11:22:40.937031: Current learning rate: 0.00822\n2024-10-16 11:25:05.516321: train_loss -0.672\n2024-10-16 11:25:05.516587: val_loss -0.7061\n2024-10-16 11:25:05.516765: Pseudo dice [0.8444, 0.8268, 0.8532, 0.7946]\n2024-10-16 11:25:05.516921: Epoch time: 144.58 s\n2024-10-16 11:25:07.962028: \n2024-10-16 11:25:07.962319: Epoch 197\n2024-10-16 11:25:07.962487: Current learning rate: 0.00821\n2024-10-16 11:27:32.449307: train_loss -0.6886\n2024-10-16 11:27:32.449592: val_loss -0.6861\n2024-10-16 11:27:32.449809: Pseudo dice [0.8431, 0.8217, 0.893, 0.6648]\n2024-10-16 11:27:32.449999: Epoch time: 144.49 s\n2024-10-16 11:27:34.893806: \n2024-10-16 11:27:34.894062: Epoch 198\n2024-10-16 11:27:34.894221: Current learning rate: 0.0082\n2024-10-16 11:29:59.764357: train_loss -0.634\n2024-10-16 11:29:59.764688: val_loss -0.6679\n2024-10-16 11:29:59.764914: Pseudo dice [0.8365, 0.8169, 0.8487, 0.7345]\n2024-10-16 11:29:59.765061: Epoch time: 144.87 s\n2024-10-16 11:30:02.425309: \n2024-10-16 11:30:02.425587: Epoch 199\n2024-10-16 11:30:02.425771: Current learning rate: 0.00819\n2024-10-16 11:32:27.115232: train_loss -0.6448\n2024-10-16 11:32:27.115569: val_loss -0.6863\n2024-10-16 11:32:27.115733: Pseudo dice [0.8434, 0.8075, 0.8745, 0.7606]\n2024-10-16 11:32:27.115854: Epoch time: 144.69 s\n2024-10-16 11:32:30.710267: \n2024-10-16 11:32:30.710550: Epoch 200\n2024-10-16 11:32:30.710729: Current learning rate: 0.00818\n2024-10-16 11:34:55.225465: train_loss -0.6598\n2024-10-16 11:34:55.225733: val_loss -0.6928\n2024-10-16 11:34:55.225917: Pseudo dice [0.8343, 0.7963, 0.8644, 0.8133]\n2024-10-16 11:34:55.226104: Epoch time: 144.52 s\n2024-10-16 11:34:58.726379: \n2024-10-16 11:34:58.726705: Epoch 201\n2024-10-16 11:34:58.726942: Current learning rate: 0.00817\n2024-10-16 11:37:23.343410: train_loss -0.6815\n2024-10-16 11:37:23.343753: val_loss -0.7104\n2024-10-16 11:37:23.343950: Pseudo dice [0.8488, 0.8412, 0.8544, 0.8257]\n2024-10-16 11:37:23.344202: Epoch time: 144.62 s\n2024-10-16 11:37:26.071874: \n2024-10-16 11:37:26.072288: Epoch 202\n2024-10-16 11:37:26.072459: Current learning rate: 0.00816\n2024-10-16 11:39:50.900921: train_loss -0.6545\n2024-10-16 11:39:50.901218: val_loss -0.6381\n2024-10-16 11:39:50.901521: Pseudo dice [0.8306, 0.8371, 0.826, 0.7156]\n2024-10-16 11:39:50.901657: Epoch time: 144.83 s\n2024-10-16 11:39:53.325625: \n2024-10-16 11:39:53.325984: Epoch 203\n2024-10-16 11:39:53.326190: Current learning rate: 0.00815\n2024-10-16 11:42:17.913047: train_loss -0.6816\n2024-10-16 11:42:17.913342: val_loss -0.6669\n2024-10-16 11:42:17.913512: Pseudo dice [0.8456, 0.7949, 0.8477, 0.8212]\n2024-10-16 11:42:17.913708: Epoch time: 144.59 s\n2024-10-16 11:42:20.338819: \n2024-10-16 11:42:20.339205: Epoch 204\n2024-10-16 11:42:20.339397: Current learning rate: 0.00814\n2024-10-16 11:44:45.284262: train_loss -0.6649\n2024-10-16 11:44:45.284627: val_loss -0.6875\n2024-10-16 11:44:45.284848: Pseudo dice [0.8483, 0.8251, 0.8812, 0.7924]\n2024-10-16 11:44:45.285041: Epoch time: 144.95 s\n2024-10-16 11:44:47.838305: \n2024-10-16 11:44:47.838688: Epoch 205\n2024-10-16 11:44:47.838866: Current learning rate: 0.00813\n2024-10-16 11:47:12.989008: train_loss -0.6673\n2024-10-16 11:47:12.989308: val_loss -0.6949\n2024-10-16 11:47:12.989753: Pseudo dice [0.8358, 0.8171, 0.8551, 0.8488]\n2024-10-16 11:47:12.989916: Epoch time: 145.15 s\n2024-10-16 11:47:15.381567: \n2024-10-16 11:47:15.381930: Epoch 206\n2024-10-16 11:47:15.382132: Current learning rate: 0.00813\n2024-10-16 11:49:40.392853: train_loss -0.6659\n2024-10-16 11:49:40.393160: val_loss -0.6915\n2024-10-16 11:49:40.393350: Pseudo dice [0.8407, 0.8369, 0.8691, 0.8163]\n2024-10-16 11:49:40.393482: Epoch time: 145.01 s\n2024-10-16 11:49:42.617484: \n2024-10-16 11:49:42.617729: Epoch 207\n2024-10-16 11:49:42.617908: Current learning rate: 0.00812\n2024-10-16 11:52:07.369408: train_loss -0.6897\n2024-10-16 11:52:07.369750: val_loss -0.7093\n2024-10-16 11:52:07.369921: Pseudo dice [0.8511, 0.8142, 0.8797, 0.8382]\n2024-10-16 11:52:07.370053: Epoch time: 144.75 s\n2024-10-16 11:52:07.370278: Yayy! New best EMA pseudo Dice: 0.8304\n2024-10-16 11:52:10.367071: \n2024-10-16 11:52:10.367350: Epoch 208\n2024-10-16 11:52:10.367491: Current learning rate: 0.00811\n2024-10-16 11:54:34.755687: train_loss -0.6791\n2024-10-16 11:54:34.755955: val_loss -0.6533\n2024-10-16 11:54:34.756126: Pseudo dice [0.8239, 0.8214, 0.8598, 0.7969]\n2024-10-16 11:54:34.756307: Epoch time: 144.39 s\n2024-10-16 11:54:37.197573: \n2024-10-16 11:54:37.197841: Epoch 209\n2024-10-16 11:54:37.198116: Current learning rate: 0.0081\n2024-10-16 11:57:01.605441: train_loss -0.6735\n2024-10-16 11:57:01.605801: val_loss -0.6811\n2024-10-16 11:57:01.605980: Pseudo dice [0.8558, 0.8226, 0.872, 0.7102]\n2024-10-16 11:57:01.606258: Epoch time: 144.41 s\n2024-10-16 11:57:03.834792: \n2024-10-16 11:57:03.834979: Epoch 210\n2024-10-16 11:57:03.835167: Current learning rate: 0.00809\n2024-10-16 11:59:28.288132: train_loss -0.674\n2024-10-16 11:59:28.288422: val_loss -0.7137\n2024-10-16 11:59:28.288612: Pseudo dice [0.8392, 0.8449, 0.8597, 0.7497]\n2024-10-16 11:59:28.288820: Epoch time: 144.46 s\n2024-10-16 11:59:30.500351: \n2024-10-16 11:59:30.500681: Epoch 211\n2024-10-16 11:59:30.500858: Current learning rate: 0.00808\n2024-10-16 12:01:55.111988: train_loss -0.6775\n2024-10-16 12:01:55.112308: val_loss -0.6867\n2024-10-16 12:01:55.112531: Pseudo dice [0.834, 0.7902, 0.8872, 0.8341]\n2024-10-16 12:01:55.112751: Epoch time: 144.61 s\n2024-10-16 12:01:57.398769: \n2024-10-16 12:01:57.399114: Epoch 212\n2024-10-16 12:01:57.399293: Current learning rate: 0.00807\n2024-10-16 12:04:22.129952: train_loss -0.6535\n2024-10-16 12:04:22.130376: val_loss -0.6785\n2024-10-16 12:04:22.130620: Pseudo dice [0.8305, 0.8044, 0.8628, 0.6333]\n2024-10-16 12:04:22.130774: Epoch time: 144.73 s\n2024-10-16 12:04:24.523128: \n2024-10-16 12:04:24.523489: Epoch 213\n2024-10-16 12:04:24.523682: Current learning rate: 0.00806\n2024-10-16 12:06:49.201976: train_loss -0.6624\n2024-10-16 12:06:49.202253: val_loss -0.6655\n2024-10-16 12:06:49.202462: Pseudo dice [0.8277, 0.8015, 0.8706, 0.7635]\n2024-10-16 12:06:49.202653: Epoch time: 144.68 s\n2024-10-16 12:06:51.496313: \n2024-10-16 12:06:51.496579: Epoch 214\n2024-10-16 12:06:51.496765: Current learning rate: 0.00805\n2024-10-16 12:09:16.229108: train_loss -0.6604\n2024-10-16 12:09:16.229466: val_loss -0.7171\n2024-10-16 12:09:16.229643: Pseudo dice [0.8397, 0.8412, 0.8877, 0.7984]\n2024-10-16 12:09:16.229761: Epoch time: 144.74 s\n2024-10-16 12:09:18.529945: \n2024-10-16 12:09:18.530274: Epoch 215\n2024-10-16 12:09:18.530474: Current learning rate: 0.00804\n2024-10-16 12:11:43.157559: train_loss -0.6678\n2024-10-16 12:11:43.157866: val_loss -0.701\n2024-10-16 12:11:43.158051: Pseudo dice [0.845, 0.8301, 0.8661, 0.8801]\n2024-10-16 12:11:43.158240: Epoch time: 144.63 s\n2024-10-16 12:11:45.449665: \n2024-10-16 12:11:45.449948: Epoch 216\n2024-10-16 12:11:45.450148: Current learning rate: 0.00803\n2024-10-16 12:14:10.443583: train_loss -0.6738\n2024-10-16 12:14:10.443892: val_loss -0.7089\n2024-10-16 12:14:10.444037: Pseudo dice [0.8463, 0.8195, 0.8709, 0.8506]\n2024-10-16 12:14:10.444176: Epoch time: 145.0 s\n2024-10-16 12:14:12.712436: \n2024-10-16 12:14:12.712690: Epoch 217\n2024-10-16 12:14:12.712866: Current learning rate: 0.00802\n2024-10-16 12:16:37.764172: train_loss -0.6437\n2024-10-16 12:16:37.764515: val_loss -0.6763\n2024-10-16 12:16:37.764711: Pseudo dice [0.8502, 0.8137, 0.8741, 0.8181]\n2024-10-16 12:16:37.764855: Epoch time: 145.05 s\n2024-10-16 12:16:37.765105: Yayy! New best EMA pseudo Dice: 0.831\n2024-10-16 12:16:40.987500: \n2024-10-16 12:16:40.987833: Epoch 218\n2024-10-16 12:16:40.988015: Current learning rate: 0.00801\n2024-10-16 12:19:05.886906: train_loss -0.6639\n2024-10-16 12:19:05.887466: val_loss -0.674\n2024-10-16 12:19:05.887701: Pseudo dice [0.8308, 0.8235, 0.859, 0.7468]\n2024-10-16 12:19:05.887824: Epoch time: 144.9 s\n2024-10-16 12:19:08.218546: \n2024-10-16 12:19:08.218876: Epoch 219\n2024-10-16 12:19:08.219026: Current learning rate: 0.00801\n2024-10-16 12:21:33.086427: train_loss -0.6604\n2024-10-16 12:21:33.086707: val_loss -0.6892\n2024-10-16 12:21:33.086854: Pseudo dice [0.8393, 0.8138, 0.8796, 0.7473]\n2024-10-16 12:21:33.087146: Epoch time: 144.87 s\n2024-10-16 12:21:35.474660: \n2024-10-16 12:21:35.475025: Epoch 220\n2024-10-16 12:21:35.475247: Current learning rate: 0.008\n2024-10-16 12:24:00.422623: train_loss -0.6581\n2024-10-16 12:24:00.423114: val_loss -0.6635\n2024-10-16 12:24:00.423295: Pseudo dice [0.8373, 0.8189, 0.8515, 0.7387]\n2024-10-16 12:24:00.423412: Epoch time: 144.95 s\n2024-10-16 12:24:02.730401: \n2024-10-16 12:24:02.730571: Epoch 221\n2024-10-16 12:24:02.730741: Current learning rate: 0.00799\n2024-10-16 12:26:27.784706: train_loss -0.6514\n2024-10-16 12:26:27.785051: val_loss -0.6498\n2024-10-16 12:26:27.785327: Pseudo dice [0.8277, 0.7939, 0.8663, 0.7364]\n2024-10-16 12:26:27.785471: Epoch time: 145.06 s\n2024-10-16 12:26:31.279396: \n2024-10-16 12:26:31.279798: Epoch 222\n2024-10-16 12:26:31.279988: Current learning rate: 0.00798\n2024-10-16 12:28:56.504389: train_loss -0.6283\n2024-10-16 12:28:56.504721: val_loss -0.6649\n2024-10-16 12:28:56.504937: Pseudo dice [0.8291, 0.8102, 0.852, 0.7792]\n2024-10-16 12:28:56.505076: Epoch time: 145.23 s\n2024-10-16 12:28:58.886889: \n2024-10-16 12:28:58.887141: Epoch 223\n2024-10-16 12:28:58.887293: Current learning rate: 0.00797\n2024-10-16 12:31:23.964412: train_loss -0.6604\n2024-10-16 12:31:23.964681: val_loss -0.6384\n2024-10-16 12:31:23.964897: Pseudo dice [0.8264, 0.7659, 0.8665, 0.6225]\n2024-10-16 12:31:23.965075: Epoch time: 145.08 s\n2024-10-16 12:31:26.248421: \n2024-10-16 12:31:26.248777: Epoch 224\n2024-10-16 12:31:26.248919: Current learning rate: 0.00796\n2024-10-16 12:33:51.810517: train_loss -0.6616\n2024-10-16 12:33:51.810797: val_loss -0.6811\n2024-10-16 12:33:51.811006: Pseudo dice [0.8262, 0.8269, 0.8637, 0.85]\n2024-10-16 12:33:51.811225: Epoch time: 145.56 s\n2024-10-16 12:33:54.241669: \n2024-10-16 12:33:54.241867: Epoch 225\n2024-10-16 12:33:54.242028: Current learning rate: 0.00795\n2024-10-16 12:36:19.419104: train_loss -0.6851\n2024-10-16 12:36:19.419475: val_loss -0.6653\n2024-10-16 12:36:19.419716: Pseudo dice [0.8413, 0.8154, 0.864, 0.7893]\n2024-10-16 12:36:19.419904: Epoch time: 145.18 s\n2024-10-16 12:36:21.908927: \n2024-10-16 12:36:21.909194: Epoch 226\n2024-10-16 12:36:21.909390: Current learning rate: 0.00794\n2024-10-16 12:38:46.867193: train_loss -0.6882\n2024-10-16 12:38:46.867490: val_loss -0.6827\n2024-10-16 12:38:46.867735: Pseudo dice [0.8539, 0.8407, 0.8814, 0.8315]\n2024-10-16 12:38:46.867878: Epoch time: 144.96 s\n2024-10-16 12:38:49.153926: \n2024-10-16 12:38:49.154292: Epoch 227\n2024-10-16 12:38:49.154479: Current learning rate: 0.00793\n2024-10-16 12:41:14.102583: train_loss -0.6959\n2024-10-16 12:41:14.102950: val_loss -0.6957\n2024-10-16 12:41:14.103179: Pseudo dice [0.8526, 0.8404, 0.8872, 0.824]\n2024-10-16 12:41:14.103384: Epoch time: 144.95 s\n2024-10-16 12:41:16.470499: \n2024-10-16 12:41:16.470807: Epoch 228\n2024-10-16 12:41:16.470980: Current learning rate: 0.00792\n2024-10-16 12:43:41.441628: train_loss -0.6822\n2024-10-16 12:43:41.442142: val_loss -0.7091\n2024-10-16 12:43:41.442375: Pseudo dice [0.8378, 0.8418, 0.8519, 0.7798]\n2024-10-16 12:43:41.442575: Epoch time: 144.97 s\n2024-10-16 12:43:43.735085: \n2024-10-16 12:43:43.735328: Epoch 229\n2024-10-16 12:43:43.735512: Current learning rate: 0.00791\n2024-10-16 12:46:08.843402: train_loss -0.6606\n2024-10-16 12:46:08.843749: val_loss -0.6683\n2024-10-16 12:46:08.843958: Pseudo dice [0.8515, 0.8166, 0.869, 0.6441]\n2024-10-16 12:46:08.844129: Epoch time: 145.11 s\n2024-10-16 12:46:11.038524: \n2024-10-16 12:46:11.038874: Epoch 230\n2024-10-16 12:46:11.039043: Current learning rate: 0.0079\n2024-10-16 12:48:36.064776: train_loss -0.6808\n2024-10-16 12:48:36.065071: val_loss -0.6778\n2024-10-16 12:48:36.065290: Pseudo dice [0.8311, 0.8216, 0.8979, 0.8098]\n2024-10-16 12:48:36.065488: Epoch time: 145.03 s\n2024-10-16 12:48:38.431523: \n2024-10-16 12:48:38.431874: Epoch 231\n2024-10-16 12:48:38.432047: Current learning rate: 0.00789\n2024-10-16 12:51:03.309356: train_loss -0.6648\n2024-10-16 12:51:03.309702: val_loss -0.6815\n2024-10-16 12:51:03.309897: Pseudo dice [0.8444, 0.8088, 0.8786, 0.7915]\n2024-10-16 12:51:03.310021: Epoch time: 144.88 s\n2024-10-16 12:51:05.656544: \n2024-10-16 12:51:05.656851: Epoch 232\n2024-10-16 12:51:05.657046: Current learning rate: 0.00789\n2024-10-16 12:53:30.647403: train_loss -0.6671\n2024-10-16 12:53:30.647647: val_loss -0.672\n2024-10-16 12:53:30.647841: Pseudo dice [0.8331, 0.8237, 0.8818, 0.6194]\n2024-10-16 12:53:30.648009: Epoch time: 144.99 s\n2024-10-16 12:53:33.077682: \n2024-10-16 12:53:33.077899: Epoch 233\n2024-10-16 12:53:33.078070: Current learning rate: 0.00788\n2024-10-16 12:55:58.390481: train_loss -0.6604\n2024-10-16 12:55:58.390850: val_loss -0.7066\n2024-10-16 12:55:58.391019: Pseudo dice [0.846, 0.8289, 0.8527, 0.7589]\n2024-10-16 12:55:58.391160: Epoch time: 145.32 s\n2024-10-16 12:56:00.697373: \n2024-10-16 12:56:00.697814: Epoch 234\n2024-10-16 12:56:00.698004: Current learning rate: 0.00787\n2024-10-16 12:58:25.691333: train_loss -0.6727\n2024-10-16 12:58:25.691652: val_loss -0.6919\n2024-10-16 12:58:25.691828: Pseudo dice [0.845, 0.7943, 0.884, 0.7408]\n2024-10-16 12:58:25.691959: Epoch time: 145.0 s\n2024-10-16 12:58:27.909755: \n2024-10-16 12:58:27.910055: Epoch 235\n2024-10-16 12:58:27.910234: Current learning rate: 0.00786\n2024-10-16 13:00:52.728542: train_loss -0.6547\n2024-10-16 13:00:52.728969: val_loss -0.6783\n2024-10-16 13:00:52.729161: Pseudo dice [0.8556, 0.8192, 0.8398, 0.7831]\n2024-10-16 13:00:52.729316: Epoch time: 144.82 s\n2024-10-16 13:00:54.961757: \n2024-10-16 13:00:54.962062: Epoch 236\n2024-10-16 13:00:54.962258: Current learning rate: 0.00785\n2024-10-16 13:03:19.707268: train_loss -0.685\n2024-10-16 13:03:19.707562: val_loss -0.6851\n2024-10-16 13:03:19.707814: Pseudo dice [0.8213, 0.8234, 0.8561, 0.7775]\n2024-10-16 13:03:19.707988: Epoch time: 144.75 s\n2024-10-16 13:03:21.957264: \n2024-10-16 13:03:21.957481: Epoch 237\n2024-10-16 13:03:21.957631: Current learning rate: 0.00784\n2024-10-16 13:05:46.909934: train_loss -0.6943\n2024-10-16 13:05:46.910361: val_loss -0.7324\n2024-10-16 13:05:46.910544: Pseudo dice [0.8608, 0.8488, 0.8913, 0.8291]\n2024-10-16 13:05:46.910667: Epoch time: 144.96 s\n2024-10-16 13:05:49.157381: \n2024-10-16 13:05:49.157725: Epoch 238\n2024-10-16 13:05:49.157881: Current learning rate: 0.00783\n2024-10-16 13:08:13.852706: train_loss -0.6933\n2024-10-16 13:08:13.853000: val_loss -0.6683\n2024-10-16 13:08:13.853225: Pseudo dice [0.8441, 0.813, 0.8701, 0.7042]\n2024-10-16 13:08:13.853423: Epoch time: 144.7 s\n2024-10-16 13:08:16.150201: \n2024-10-16 13:08:16.150386: Epoch 239\n2024-10-16 13:08:16.150559: Current learning rate: 0.00782\n2024-10-16 13:10:41.070406: train_loss -0.6779\n2024-10-16 13:10:41.070713: val_loss -0.6759\n2024-10-16 13:10:41.070902: Pseudo dice [0.8432, 0.8004, 0.8798, 0.7935]\n2024-10-16 13:10:41.071084: Epoch time: 144.92 s\n2024-10-16 13:10:43.390339: \n2024-10-16 13:10:43.390513: Epoch 240\n2024-10-16 13:10:43.390652: Current learning rate: 0.00781\n2024-10-16 13:13:08.343465: train_loss -0.6857\n2024-10-16 13:13:08.343745: val_loss -0.6958\n2024-10-16 13:13:08.343987: Pseudo dice [0.8189, 0.843, 0.8634, 0.8383]\n2024-10-16 13:13:08.344154: Epoch time: 144.96 s\n2024-10-16 13:13:10.844768: \n2024-10-16 13:13:10.845062: Epoch 241\n2024-10-16 13:13:10.845274: Current learning rate: 0.0078\n2024-10-16 13:15:35.875561: train_loss -0.6749\n2024-10-16 13:15:35.875939: val_loss -0.699\n2024-10-16 13:15:35.876173: Pseudo dice [0.8425, 0.8431, 0.8618, 0.8342]\n2024-10-16 13:15:35.876344: Epoch time: 145.03 s\n2024-10-16 13:15:38.437956: \n2024-10-16 13:15:38.438178: Epoch 242\n2024-10-16 13:15:38.438349: Current learning rate: 0.00779\n2024-10-16 13:18:03.706431: train_loss -0.6895\n2024-10-16 13:18:03.706777: val_loss -0.6839\n2024-10-16 13:18:03.707047: Pseudo dice [0.8395, 0.8464, 0.8657, 0.6997]\n2024-10-16 13:18:03.707220: Epoch time: 145.27 s\n2024-10-16 13:18:05.992084: \n2024-10-16 13:18:05.992430: Epoch 243\n2024-10-16 13:18:05.992637: Current learning rate: 0.00778\n2024-10-16 13:20:31.066301: train_loss -0.6938\n2024-10-16 13:20:31.066589: val_loss -0.6998\n2024-10-16 13:20:31.066821: Pseudo dice [0.8579, 0.8367, 0.8852, 0.8396]\n2024-10-16 13:20:31.066967: Epoch time: 145.08 s\n2024-10-16 13:20:34.590496: \n2024-10-16 13:20:34.590958: Epoch 244\n2024-10-16 13:20:34.591162: Current learning rate: 0.00777\n2024-10-16 13:22:59.800823: train_loss -0.6847\n2024-10-16 13:22:59.801174: val_loss -0.7156\n2024-10-16 13:22:59.801323: Pseudo dice [0.8575, 0.8547, 0.8752, 0.8416]\n2024-10-16 13:22:59.801434: Epoch time: 145.21 s\n2024-10-16 13:22:59.801519: Yayy! New best EMA pseudo Dice: 0.832\n2024-10-16 13:23:03.811490: \n2024-10-16 13:23:03.811823: Epoch 245\n2024-10-16 13:23:03.811987: Current learning rate: 0.00777\n2024-10-16 13:25:28.792117: train_loss -0.6909\n2024-10-16 13:25:28.792362: val_loss -0.7314\n2024-10-16 13:25:28.792558: Pseudo dice [0.8577, 0.8537, 0.8858, 0.8819]\n2024-10-16 13:25:28.792712: Epoch time: 144.98 s\n2024-10-16 13:25:28.792819: Yayy! New best EMA pseudo Dice: 0.8358\n2024-10-16 13:25:31.938296: \n2024-10-16 13:25:31.938709: Epoch 246\n2024-10-16 13:25:31.938890: Current learning rate: 0.00776\n2024-10-16 13:27:57.092194: train_loss -0.6959\n2024-10-16 13:27:57.092666: val_loss -0.6965\n2024-10-16 13:27:57.092867: Pseudo dice [0.8471, 0.8296, 0.8735, 0.8398]\n2024-10-16 13:27:57.093040: Epoch time: 145.16 s\n2024-10-16 13:27:57.093195: Yayy! New best EMA pseudo Dice: 0.8369\n2024-10-16 13:28:00.444532: \n2024-10-16 13:28:00.444856: Epoch 247\n2024-10-16 13:28:00.445050: Current learning rate: 0.00775\n2024-10-16 13:30:25.525682: train_loss -0.6892\n2024-10-16 13:30:25.526076: val_loss -0.6889\n2024-10-16 13:30:25.526314: Pseudo dice [0.849, 0.8392, 0.8899, 0.8331]\n2024-10-16 13:30:25.526578: Epoch time: 145.08 s\n2024-10-16 13:30:25.526687: Yayy! New best EMA pseudo Dice: 0.8385\n2024-10-16 13:30:28.730970: \n2024-10-16 13:30:28.731375: Epoch 248\n2024-10-16 13:30:28.731545: Current learning rate: 0.00774\n2024-10-16 13:32:53.907259: train_loss -0.6796\n2024-10-16 13:32:53.907714: val_loss -0.7219\n2024-10-16 13:32:53.907918: Pseudo dice [0.8599, 0.8261, 0.875, 0.8884]\n2024-10-16 13:32:53.908055: Epoch time: 145.18 s\n2024-10-16 13:32:53.908217: Yayy! New best EMA pseudo Dice: 0.8409\n2024-10-16 13:32:57.027705: \n2024-10-16 13:32:57.028037: Epoch 249\n2024-10-16 13:32:57.028265: Current learning rate: 0.00773\n2024-10-16 13:35:21.802436: train_loss -0.6616\n2024-10-16 13:35:21.802725: val_loss -0.6911\n2024-10-16 13:35:21.802926: Pseudo dice [0.8338, 0.8291, 0.8849, 0.757]\n2024-10-16 13:35:21.803066: Epoch time: 144.78 s\n2024-10-16 13:35:24.962371: \n2024-10-16 13:35:24.962692: Epoch 250\n2024-10-16 13:35:24.962888: Current learning rate: 0.00772\n2024-10-16 13:37:50.045392: train_loss -0.6673\n2024-10-16 13:37:50.045678: val_loss -0.6914\n2024-10-16 13:37:50.045870: Pseudo dice [0.8375, 0.8237, 0.8853, 0.7814]\n2024-10-16 13:37:50.046049: Epoch time: 145.09 s\n2024-10-16 13:37:52.269499: \n2024-10-16 13:37:52.269862: Epoch 251\n2024-10-16 13:37:52.270046: Current learning rate: 0.00771\n2024-10-16 13:40:16.811331: train_loss -0.6877\n2024-10-16 13:40:16.811660: val_loss -0.6755\n2024-10-16 13:40:16.811852: Pseudo dice [0.8361, 0.8275, 0.8584, 0.8011]\n2024-10-16 13:40:16.812053: Epoch time: 144.54 s\n2024-10-16 13:40:19.125295: \n2024-10-16 13:40:19.125658: Epoch 252\n2024-10-16 13:40:19.125799: Current learning rate: 0.0077\n2024-10-16 13:42:43.523270: train_loss -0.6967\n2024-10-16 13:42:43.523564: val_loss -0.7033\n2024-10-16 13:42:43.523801: Pseudo dice [0.8463, 0.848, 0.8715, 0.8564]\n2024-10-16 13:42:43.523968: Epoch time: 144.4 s\n2024-10-16 13:42:45.752532: \n2024-10-16 13:42:45.752825: Epoch 253\n2024-10-16 13:42:45.753004: Current learning rate: 0.00769\n2024-10-16 13:45:10.227672: train_loss -0.6965\n2024-10-16 13:45:10.227945: val_loss -0.6855\n2024-10-16 13:45:10.228163: Pseudo dice [0.8457, 0.8414, 0.8845, 0.7971]\n2024-10-16 13:45:10.228325: Epoch time: 144.48 s\n2024-10-16 13:45:12.529939: \n2024-10-16 13:45:12.530290: Epoch 254\n2024-10-16 13:45:12.530451: Current learning rate: 0.00768\n2024-10-16 13:47:36.980205: train_loss -0.6829\n2024-10-16 13:47:36.980546: val_loss -0.7035\n2024-10-16 13:47:36.980722: Pseudo dice [0.8426, 0.8167, 0.8751, 0.8128]\n2024-10-16 13:47:36.980853: Epoch time: 144.45 s\n2024-10-16 13:47:39.219157: \n2024-10-16 13:47:39.219486: Epoch 255\n2024-10-16 13:47:39.219634: Current learning rate: 0.00767\n2024-10-16 13:50:03.691894: train_loss -0.6852\n2024-10-16 13:50:03.692241: val_loss -0.7203\n2024-10-16 13:50:03.692450: Pseudo dice [0.855, 0.8461, 0.8862, 0.8114]\n2024-10-16 13:50:03.692644: Epoch time: 144.48 s\n2024-10-16 13:50:06.139164: \n2024-10-16 13:50:06.139527: Epoch 256\n2024-10-16 13:50:06.139720: Current learning rate: 0.00766\n2024-10-16 13:52:30.661579: train_loss -0.6804\n2024-10-16 13:52:30.661998: val_loss -0.6981\n2024-10-16 13:52:30.662248: Pseudo dice [0.8572, 0.8375, 0.8888, 0.8364]\n2024-10-16 13:52:30.662429: Epoch time: 144.53 s\n2024-10-16 13:52:30.662580: Yayy! New best EMA pseudo Dice: 0.842\n2024-10-16 13:52:33.817849: \n2024-10-16 13:52:33.818239: Epoch 257\n2024-10-16 13:52:33.818432: Current learning rate: 0.00765\n2024-10-16 13:54:58.403006: train_loss -0.6782\n2024-10-16 13:54:58.403311: val_loss -0.7084\n2024-10-16 13:54:58.403487: Pseudo dice [0.8528, 0.8325, 0.882, 0.7856]\n2024-10-16 13:54:58.403620: Epoch time: 144.59 s\n2024-10-16 13:55:00.649515: \n2024-10-16 13:55:00.649709: Epoch 258\n2024-10-16 13:55:00.649846: Current learning rate: 0.00764\n2024-10-16 13:57:25.171302: train_loss -0.6696\n2024-10-16 13:57:25.171582: val_loss -0.699\n2024-10-16 13:57:25.171763: Pseudo dice [0.8502, 0.8123, 0.8753, 0.7766]\n2024-10-16 13:57:25.171958: Epoch time: 144.52 s\n2024-10-16 13:57:27.535678: \n2024-10-16 13:57:27.536111: Epoch 259\n2024-10-16 13:57:27.536351: Current learning rate: 0.00764\n2024-10-16 13:59:52.432325: train_loss -0.6678\n2024-10-16 13:59:52.432662: val_loss -0.6769\n2024-10-16 13:59:52.432836: Pseudo dice [0.8484, 0.8211, 0.8851, 0.7501]\n2024-10-16 13:59:52.433010: Epoch time: 144.9 s\n2024-10-16 13:59:54.737541: \n2024-10-16 13:59:54.737868: Epoch 260\n2024-10-16 13:59:54.738202: Current learning rate: 0.00763\n2024-10-16 14:02:19.550515: train_loss -0.6932\n2024-10-16 14:02:19.550821: val_loss -0.7207\n2024-10-16 14:02:19.550957: Pseudo dice [0.8552, 0.8364, 0.8941, 0.842]\n2024-10-16 14:02:19.551067: Epoch time: 144.82 s\n2024-10-16 14:02:21.880171: \n2024-10-16 14:02:21.880502: Epoch 261\n2024-10-16 14:02:21.880674: Current learning rate: 0.00762\n2024-10-16 14:04:46.727542: train_loss -0.6894\n2024-10-16 14:04:46.727832: val_loss -0.6947\n2024-10-16 14:04:46.728036: Pseudo dice [0.8487, 0.8415, 0.899, 0.7697]\n2024-10-16 14:04:46.728234: Epoch time: 144.85 s\n2024-10-16 14:04:49.047381: \n2024-10-16 14:04:49.047556: Epoch 262\n2024-10-16 14:04:49.047732: Current learning rate: 0.00761\n2024-10-16 14:07:13.906046: train_loss -0.6957\n2024-10-16 14:07:13.906513: val_loss -0.6862\n2024-10-16 14:07:13.906698: Pseudo dice [0.8509, 0.839, 0.8789, 0.8295]\n2024-10-16 14:07:13.906832: Epoch time: 144.86 s\n2024-10-16 14:07:16.288393: \n2024-10-16 14:07:16.288683: Epoch 263\n2024-10-16 14:07:16.289071: Current learning rate: 0.0076\n2024-10-16 14:09:41.179485: train_loss -0.6819\n2024-10-16 14:09:41.179768: val_loss -0.6923\n2024-10-16 14:09:41.179928: Pseudo dice [0.8465, 0.8178, 0.8827, 0.8532]\n2024-10-16 14:09:41.180040: Epoch time: 144.89 s\n2024-10-16 14:09:41.180162: Yayy! New best EMA pseudo Dice: 0.8424\n2024-10-16 14:09:44.288669: \n2024-10-16 14:09:44.288998: Epoch 264\n2024-10-16 14:09:44.289177: Current learning rate: 0.00759\n2024-10-16 14:12:09.157759: train_loss -0.6775\n2024-10-16 14:12:09.158069: val_loss -0.7044\n2024-10-16 14:12:09.158435: Pseudo dice [0.8529, 0.8232, 0.8838, 0.8449]\n2024-10-16 14:12:09.158700: Epoch time: 144.87 s\n2024-10-16 14:12:09.158837: Yayy! New best EMA pseudo Dice: 0.8433\n2024-10-16 14:12:13.329137: \n2024-10-16 14:12:13.329451: Epoch 265\n2024-10-16 14:12:13.329627: Current learning rate: 0.00758\n2024-10-16 14:14:38.435363: train_loss -0.6965\n2024-10-16 14:14:38.435658: val_loss -0.7407\n2024-10-16 14:14:38.435860: Pseudo dice [0.869, 0.8343, 0.8994, 0.8604]\n2024-10-16 14:14:38.436039: Epoch time: 145.11 s\n2024-10-16 14:14:38.436246: Yayy! New best EMA pseudo Dice: 0.8455\n2024-10-16 14:14:41.811312: \n2024-10-16 14:14:41.811700: Epoch 266\n2024-10-16 14:14:41.811877: Current learning rate: 0.00757\n2024-10-16 14:17:06.686038: train_loss -0.7008\n2024-10-16 14:17:06.686365: val_loss -0.7261\n2024-10-16 14:17:06.686563: Pseudo dice [0.8531, 0.8472, 0.8774, 0.8051]\n2024-10-16 14:17:06.686750: Epoch time: 144.88 s\n2024-10-16 14:17:06.686873: Yayy! New best EMA pseudo Dice: 0.8455\n2024-10-16 14:17:09.789470: \n2024-10-16 14:17:09.789812: Epoch 267\n2024-10-16 14:17:09.790007: Current learning rate: 0.00756\n2024-10-16 14:19:34.710314: train_loss -0.6947\n2024-10-16 14:19:34.710681: val_loss -0.7219\n2024-10-16 14:19:34.710866: Pseudo dice [0.8432, 0.845, 0.8889, 0.8175]\n2024-10-16 14:19:34.711043: Epoch time: 144.92 s\n2024-10-16 14:19:34.711211: Yayy! New best EMA pseudo Dice: 0.8458\n2024-10-16 14:19:37.845021: \n2024-10-16 14:19:37.845351: Epoch 268\n2024-10-16 14:19:37.845552: Current learning rate: 0.00755\n2024-10-16 14:22:02.769213: train_loss -0.6994\n2024-10-16 14:22:02.769504: val_loss -0.7073\n2024-10-16 14:22:02.769781: Pseudo dice [0.8691, 0.8483, 0.8925, 0.8084]\n2024-10-16 14:22:02.769952: Epoch time: 144.93 s\n2024-10-16 14:22:02.770107: Yayy! New best EMA pseudo Dice: 0.8467\n2024-10-16 14:22:05.899920: \n2024-10-16 14:22:05.900252: Epoch 269\n2024-10-16 14:22:05.900434: Current learning rate: 0.00754\n2024-10-16 14:24:30.531502: train_loss -0.6669\n2024-10-16 14:24:30.532005: val_loss -0.6899\n2024-10-16 14:24:30.532266: Pseudo dice [0.83, 0.832, 0.8677, 0.785]\n2024-10-16 14:24:30.532452: Epoch time: 144.63 s\n2024-10-16 14:24:32.803153: \n2024-10-16 14:24:32.803394: Epoch 270\n2024-10-16 14:24:32.803559: Current learning rate: 0.00753\n2024-10-16 14:26:57.656145: train_loss -0.6608\n2024-10-16 14:26:57.656437: val_loss -0.6662\n2024-10-16 14:26:57.656602: Pseudo dice [0.8503, 0.8142, 0.8784, 0.7193]\n2024-10-16 14:26:57.656737: Epoch time: 144.86 s\n2024-10-16 14:26:59.993036: \n2024-10-16 14:26:59.993328: Epoch 271\n2024-10-16 14:26:59.993530: Current learning rate: 0.00752\n2024-10-16 14:29:24.289006: train_loss -0.6625\n2024-10-16 14:29:24.289344: val_loss -0.6901\n2024-10-16 14:29:24.289511: Pseudo dice [0.8413, 0.788, 0.8452, 0.8051]\n2024-10-16 14:29:24.289642: Epoch time: 144.3 s\n2024-10-16 14:29:26.571928: \n2024-10-16 14:29:26.572224: Epoch 272\n2024-10-16 14:29:26.572432: Current learning rate: 0.00751\n2024-10-16 14:31:50.920038: train_loss -0.6801\n2024-10-16 14:31:50.920419: val_loss -0.7136\n2024-10-16 14:31:50.920627: Pseudo dice [0.8446, 0.8224, 0.8959, 0.8296]\n2024-10-16 14:31:50.920765: Epoch time: 144.35 s\n2024-10-16 14:31:53.191136: \n2024-10-16 14:31:53.191489: Epoch 273\n2024-10-16 14:31:53.191666: Current learning rate: 0.00751\n2024-10-16 14:34:17.705036: train_loss -0.6825\n2024-10-16 14:34:17.705306: val_loss -0.6917\n2024-10-16 14:34:17.705500: Pseudo dice [0.8457, 0.8439, 0.8801, 0.7965]\n2024-10-16 14:34:17.705681: Epoch time: 144.52 s\n2024-10-16 14:34:19.952361: \n2024-10-16 14:34:19.952592: Epoch 274\n2024-10-16 14:34:19.952759: Current learning rate: 0.0075\n2024-10-16 14:36:44.280274: train_loss -0.6958\n2024-10-16 14:36:44.280527: val_loss -0.7008\n2024-10-16 14:36:44.280678: Pseudo dice [0.8487, 0.8115, 0.8837, 0.7656]\n2024-10-16 14:36:44.280791: Epoch time: 144.33 s\n2024-10-16 14:36:46.463723: \n2024-10-16 14:36:46.463963: Epoch 275\n2024-10-16 14:36:46.464114: Current learning rate: 0.00749\n2024-10-16 14:39:10.893777: train_loss -0.6706\n2024-10-16 14:39:10.894122: val_loss -0.7068\n2024-10-16 14:39:10.894297: Pseudo dice [0.8564, 0.8344, 0.8728, 0.6868]\n2024-10-16 14:39:10.894467: Epoch time: 144.43 s\n2024-10-16 14:39:13.233173: \n2024-10-16 14:39:13.233389: Epoch 276\n2024-10-16 14:39:13.233582: Current learning rate: 0.00748\n2024-10-16 14:41:37.812610: train_loss -0.691\n2024-10-16 14:41:37.812951: val_loss -0.7255\n2024-10-16 14:41:37.813168: Pseudo dice [0.8666, 0.8466, 0.9036, 0.8021]\n2024-10-16 14:41:37.813327: Epoch time: 144.58 s\n2024-10-16 14:41:40.062476: \n2024-10-16 14:41:40.062805: Epoch 277\n2024-10-16 14:41:40.062985: Current learning rate: 0.00747\n2024-10-16 14:44:04.796967: train_loss -0.6931\n2024-10-16 14:44:04.797288: val_loss -0.7182\n2024-10-16 14:44:04.797560: Pseudo dice [0.852, 0.8156, 0.8881, 0.8063]\n2024-10-16 14:44:04.797729: Epoch time: 144.74 s\n2024-10-16 14:44:07.126441: \n2024-10-16 14:44:07.126668: Epoch 278\n2024-10-16 14:44:07.126854: Current learning rate: 0.00746\n2024-10-16 14:46:32.060340: train_loss -0.6838\n2024-10-16 14:46:32.060601: val_loss -0.7224\n2024-10-16 14:46:32.060753: Pseudo dice [0.8504, 0.8261, 0.8909, 0.8149]\n2024-10-16 14:46:32.060860: Epoch time: 144.94 s\n2024-10-16 14:46:34.289443: \n2024-10-16 14:46:34.289689: Epoch 279\n2024-10-16 14:46:34.289852: Current learning rate: 0.00745\n2024-10-16 14:48:58.984315: train_loss -0.6935\n2024-10-16 14:48:58.984602: val_loss -0.7138\n2024-10-16 14:48:58.984788: Pseudo dice [0.8414, 0.8167, 0.8774, 0.8122]\n2024-10-16 14:48:58.984954: Epoch time: 144.7 s\n2024-10-16 14:49:01.208166: \n2024-10-16 14:49:01.208359: Epoch 280\n2024-10-16 14:49:01.208681: Current learning rate: 0.00744\n2024-10-16 14:51:25.839181: train_loss -0.6964\n2024-10-16 14:51:25.839430: val_loss -0.7007\n2024-10-16 14:51:25.839607: Pseudo dice [0.8439, 0.8208, 0.8617, 0.8074]\n2024-10-16 14:51:25.839746: Epoch time: 144.63 s\n2024-10-16 14:51:28.120642: \n2024-10-16 14:51:28.120915: Epoch 281\n2024-10-16 14:51:28.121109: Current learning rate: 0.00743\n2024-10-16 14:53:52.835736: train_loss -0.6818\n2024-10-16 14:53:52.835994: val_loss -0.6857\n2024-10-16 14:53:52.836279: Pseudo dice [0.8525, 0.8449, 0.8878, 0.6225]\n2024-10-16 14:53:52.836438: Epoch time: 144.72 s\n2024-10-16 14:53:55.103551: \n2024-10-16 14:53:55.103876: Epoch 282\n2024-10-16 14:53:55.104031: Current learning rate: 0.00742\n2024-10-16 14:56:20.206851: train_loss -0.6899\n2024-10-16 14:56:20.207188: val_loss -0.6644\n2024-10-16 14:56:20.207432: Pseudo dice [0.8355, 0.8366, 0.8684, 0.809]\n2024-10-16 14:56:20.207605: Epoch time: 145.11 s\n2024-10-16 14:56:22.452579: \n2024-10-16 14:56:22.452937: Epoch 283\n2024-10-16 14:56:22.453108: Current learning rate: 0.00741\n2024-10-16 14:58:47.171447: train_loss -0.6928\n2024-10-16 14:58:47.171732: val_loss -0.6924\n2024-10-16 14:58:47.171910: Pseudo dice [0.8327, 0.8341, 0.8469, 0.8452]\n2024-10-16 14:58:47.172128: Epoch time: 144.72 s\n2024-10-16 14:58:49.469368: \n2024-10-16 14:58:49.469682: Epoch 284\n2024-10-16 14:58:49.469878: Current learning rate: 0.0074\n2024-10-16 15:01:14.518916: train_loss -0.675\n2024-10-16 15:01:14.519215: val_loss -0.7202\n2024-10-16 15:01:14.519350: Pseudo dice [0.8488, 0.8414, 0.8847, 0.798]\n2024-10-16 15:01:14.519475: Epoch time: 145.05 s\n2024-10-16 15:01:16.741610: \n2024-10-16 15:01:16.741776: Epoch 285\n2024-10-16 15:01:16.741947: Current learning rate: 0.00739\n2024-10-16 15:03:41.457657: train_loss -0.6892\n2024-10-16 15:03:41.457939: val_loss -0.7121\n2024-10-16 15:03:41.458153: Pseudo dice [0.8521, 0.824, 0.8865, 0.7889]\n2024-10-16 15:03:41.458393: Epoch time: 144.72 s\n2024-10-16 15:03:44.633324: \n2024-10-16 15:03:44.633703: Epoch 286\n2024-10-16 15:03:44.633844: Current learning rate: 0.00738\n2024-10-16 15:06:09.648248: train_loss -0.677\n2024-10-16 15:06:09.648535: val_loss -0.6797\n2024-10-16 15:06:09.648687: Pseudo dice [0.8416, 0.8474, 0.8355, 0.8214]\n2024-10-16 15:06:09.648817: Epoch time: 145.02 s\n2024-10-16 15:06:11.941858: \n2024-10-16 15:06:11.942212: Epoch 287\n2024-10-16 15:06:11.942392: Current learning rate: 0.00738\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "shutil.rmtree('/kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_3d_fullres')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-16T03:09:32.135879Z",
          "iopub.execute_input": "2024-10-16T03:09:32.136657Z",
          "iopub.status.idle": "2024-10-16T03:09:32.882277Z",
          "shell.execute_reply.started": "2024-10-16T03:09:32.136611Z",
          "shell.execute_reply": "2024-10-16T03:09:32.881235Z"
        },
        "trusted": true,
        "id": "NwK0NLGvi0Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def delete_npy_files(directory):\n",
        "    \"\"\"\n",
        "    Deletes all .npy files in the specified directory.\n",
        "\n",
        "    Args:\n",
        "        directory (str): The path to the directory containing the .npy files.\n",
        "    \"\"\"\n",
        "    # Iterate through all files in the given directory\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".npy\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            try:\n",
        "                os.remove(file_path)\n",
        "                print(f\"Deleted: {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error deleting {file_path}: {e}\")\n",
        "\n",
        "# Example usage\n",
        "directory = \"/kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d\"\n",
        "delete_npy_files(directory)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-16T03:14:55.941156Z",
          "iopub.execute_input": "2024-10-16T03:14:55.941637Z",
          "iopub.status.idle": "2024-10-16T03:14:57.075211Z",
          "shell.execute_reply.started": "2024-10-16T03:14:55.94159Z",
          "shell.execute_reply": "2024-10-16T03:14:57.07426Z"
        },
        "trusted": true,
        "id": "qtKGBOiei0Kg",
        "outputId": "0e0e65e1-47ae-423b-cedf-134eb7044a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Deleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_206.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_150_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_150.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_187.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_072_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_112_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_238_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_019.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_015.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_252.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_048_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_281.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_144.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_296.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_072.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_029_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_138_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_245_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_282.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_094.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_151_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_240.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_096.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_099_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_293.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_183.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_058.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_027.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_163_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_126.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_294_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_115.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_201_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_216.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_197.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_011.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_109.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_251_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_278.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_120_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_253_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_005_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_044.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_171.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_274_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_239_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_255.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_145_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_101.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_134_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_006.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_057.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_174_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_082_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_033.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_104.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_169_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_180.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_003.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_107.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_119.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_223.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_270.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_093_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_057_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_273.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_105.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_022.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_080.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_202.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_009.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_288_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_185_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_022_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_166_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_298.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_155_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_089.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_234.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_177_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_052_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_089_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_185.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_117_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_100.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_279_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_124_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_014.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_024_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_143.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_042_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_149.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_137.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_283_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_027_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_069_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_158.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_105_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_141_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_173_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_210_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_289_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_023.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_268.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_007_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_260.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_236_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_243_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_186_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_123_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_044_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_018.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_078.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_100_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_280.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_012.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_218_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_047_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_235_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_168.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_142_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_300_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_280_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_187_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_137_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_282_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_261_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_126_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_028.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_295_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_116.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_095_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_103_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_128.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_121_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_049.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_138.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_077_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_188_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_190.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_203_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_269_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_297.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_160.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_292_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_193.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_036.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_231.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_231_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_001.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_268_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_205.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_010_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_215_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_087.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_212.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_230_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_094_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_030_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_162.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_025.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_247_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_251.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_175_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_233.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_077.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_116_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_049_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_288.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_048.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_098_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_017.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_277_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_056_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_225_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_002.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_213.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_091.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_264_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_284.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_219_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_170.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_298_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_018_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_043_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_052.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_285.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_283.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_179_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_108.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_133_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_014_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_292.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_210.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_184.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_233_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_229.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_255_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_201.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_120.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_270_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_083.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_075.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_259.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_192.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_248.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_118_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_291.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_299.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_102_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_042.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_194.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_244.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_109_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_147_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_153.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_028_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_128_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_279.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_179.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_043.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_108_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_248_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_129_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_154_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_265.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_098.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_181.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_146.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_095.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_051_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_182.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_250_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_227_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_254.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_236.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_259_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_206_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_160_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_026.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_031_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_226.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_299_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_214_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_183_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_266.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_190_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_276_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_162_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_051.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_168_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_084.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_240_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_034.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_083_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_154.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_008.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_244_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_252_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_204_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_197_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_129.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_005.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_287.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_133.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_002_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_234_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_199.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_016_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_091_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_115_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_274.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_114_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_073.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_220_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_211.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_227.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_032_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_024.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_275_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_290.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_196_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_184_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_263_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_266_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_218.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_020_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_191.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_020.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_029.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_275.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_041_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_016.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_011_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_114.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_180_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_125.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_034_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_228.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_136_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_135.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_131.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_204.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_243.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_272_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_189.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_069.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_035.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_003_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_188.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_223_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_194_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_033_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_119_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_178_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_132.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_015_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_036_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_181_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_021.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_232.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_294.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_209_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_040_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_025_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_205_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_040.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_269.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_242.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_038_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_073_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_291_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_296_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_178.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_067.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_191_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_202_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_247.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_155.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_216_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_068_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_152.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_232_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_068.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_169.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_237.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_271.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_065_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_176_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_159.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_023_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_081_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_224_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_176.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_144_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_030.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_117.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_047.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_102.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_123.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_242_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_006_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_035_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_281_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_053_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_213_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_284_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_246_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_038.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_262.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_293_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_239.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_132_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_135_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_167_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_081.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_276.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_260_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_161.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_237_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_097_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_008_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_159_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_258.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_221.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_295.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_141.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_300.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_189_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_149_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_092_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_111_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_147.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_001_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_264.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_262_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_171_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_104_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_182_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_165.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_146_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_140_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_285_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_257_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_170_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_039.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_065.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_273_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_056.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_122.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_289.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_161_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_225.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_122_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_214.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_235.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_099.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_198.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_203.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_087_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_075_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_230.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_267.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_131_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_163.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_172.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_148.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_186.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_066.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_253.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_066_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_103.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_246.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_032.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_256.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_287_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_121.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_217.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_238.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_217_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_172_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_084_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_082.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_142.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_093.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_286_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_136.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_013.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_124.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_017_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_101_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_208_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_004.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_215.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_110.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_007.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_041.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_192_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_096_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_140.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_145.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_221_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_004_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_207.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_153_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_199_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_271_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_254_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_257.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_177.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_143_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_277.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_224.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_092.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_013_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_167.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_290_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_219.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_165_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_118.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_009_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_278_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_112.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_267_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_021_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_166.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_148_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_258_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_152_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_193_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_175.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_173.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_078_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_250.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_010.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_265_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_026_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_158_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_125_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_110_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_220.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_058_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_228_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_229_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_272.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_107_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_208.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_196.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_245.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_209.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_226_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_097.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_256_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_134.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_211_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_031.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_261.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_207_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_067_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_039_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_151.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_019_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_297_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_212_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_198_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_012_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_263.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_080_seg.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_053.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_286.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_111.npy\nDeleted: /kaggle/working/preprocessed/Dataset013_Uterus/nnUNetPlans_2d/UMD_174.npy\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install triton"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-16T03:13:51.66554Z",
          "iopub.execute_input": "2024-10-16T03:13:51.66594Z",
          "iopub.status.idle": "2024-10-16T03:14:10.754307Z",
          "shell.execute_reply.started": "2024-10-16T03:13:51.665904Z",
          "shell.execute_reply": "2024-10-16T03:14:10.753115Z"
        },
        "trusted": true,
        "id": "FkG3uxefi0Kg",
        "outputId": "6b27c4e5-546a-4f19-c99a-90ce2b1f2614"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting triton\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton) (3.15.1)\nDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton\nSuccessfully installed triton-3.1.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nnUNetv2_export_model_to_zip -d 013 -o Uterus.zip"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-16T16:37:05.732875Z",
          "iopub.execute_input": "2024-10-16T16:37:05.733865Z",
          "iopub.status.idle": "2024-10-16T16:37:07.173762Z",
          "shell.execute_reply.started": "2024-10-16T16:37:05.733802Z",
          "shell.execute_reply": "2024-10-16T16:37:07.172208Z"
        },
        "trusted": true,
        "id": "HScJNfM-i0Kg",
        "outputId": "0c9b22e0-9074-4b40-9cce-002d845f9ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Configuration 3d_lowres\nTraceback (most recent call last):\n  File \"/opt/conda/bin/nnUNetv2_export_model_to_zip\", line 8, in <module>\n    sys.exit(export_pretrained_model_entry())\n  File \"/opt/conda/lib/python3.10/site-packages/nnunetv2/model_sharing/entry_points.py\", line 59, in export_pretrained_model_entry\n    export_pretrained_model(dataset_name_or_id=args.d, output_file=args.o, configurations=args.c, trainer=args.tr,\n  File \"/opt/conda/lib/python3.10/site-packages/nnunetv2/model_sharing/model_export.py\", line 22, in export_pretrained_model\n    raise RuntimeError(f\"{dataset_name} is missing the trained model of configuration {c}\")\nRuntimeError: Dataset013_Uterus is missing the trained model of configuration 3d_lowres\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}